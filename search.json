[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001_Brands & LLMs/branding.html",
    "href": "posts/001_Brands & LLMs/branding.html",
    "title": "Branding with LLMs",
    "section": "",
    "text": "COMM4190 ¬∑ Prompt/Response & Readings Blog\nWeek 1 ‚Äî ‚ÄúBranding & Creativity‚Äù\n\n\n\n\nIdioms.png\n\n\n\n\n\nI still remember sitting in a caf√© brainstorming names for a startup with two friends.\nOne suggested ‚ÄúCloudJar,‚Äù another ‚ÄúNeoNest,‚Äù and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity ‚Äî or would it just spit out clich√©s?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n\nPrompt: ‚ÄúGenerate 5 product names and a tagline.‚Äù\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Nature‚Äôs Sip\nTagline: ‚ÄúDrink clean. Live green.‚Äù\nNot bad. But some names feel generic ‚Äî a reminder that LLMs lean on frequency patterns.\nStill, ‚ÄúPureSip‚Äù hits the ear nicely, the kind of name you‚Äôd actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clich√©s like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - ‚ÄúRaincatcher‚Äù\n- ‚ÄúBlue Whisper‚Äù\n- ‚ÄúEverflow‚Äù\nThese feel more imaginative and less like a corporate intern‚Äôs PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: ‚ÄúGenerate names and slogans for a study app that uses AI.‚Äù\nOutput (sample): - BrainBoost ‚Äî ‚ÄúStudy smarter, not harder.‚Äù\n- MindMate ‚Äî ‚ÄúYour AI study buddy.‚Äù\n- LearnLeap ‚Äî ‚ÄúJump ahead in class.‚Äù\nThese are serviceable, but they echo phrases I‚Äôve seen on real ads.\nIt‚Äôs as if the model is channeling the marketing canon ‚Äî solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested ‚ÄúQuiztopher Nolan‚Äù for a study tool.\nIt was absurd ‚Äî and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, ‚ÄúThat‚Äôs so dumb it might just work.‚Äù\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: ‚ÄúLight speaks in silence.‚Äù\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the model‚Äôs favor.\n\n\n\n\n\nGeneric Baseline\nWithout constraints, names drift toward clich√©s.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/001_Brands & LLMs/branding.html#the-branding-experiment",
    "href": "posts/001_Brands & LLMs/branding.html#the-branding-experiment",
    "title": "Branding with LLMs",
    "section": "",
    "text": "I still remember sitting in a caf√© brainstorming names for a startup with two friends.\nOne suggested ‚ÄúCloudJar,‚Äù another ‚ÄúNeoNest,‚Äù and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity ‚Äî or would it just spit out clich√©s?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n\nPrompt: ‚ÄúGenerate 5 product names and a tagline.‚Äù\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Nature‚Äôs Sip\nTagline: ‚ÄúDrink clean. Live green.‚Äù\nNot bad. But some names feel generic ‚Äî a reminder that LLMs lean on frequency patterns.\nStill, ‚ÄúPureSip‚Äù hits the ear nicely, the kind of name you‚Äôd actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clich√©s like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - ‚ÄúRaincatcher‚Äù\n- ‚ÄúBlue Whisper‚Äù\n- ‚ÄúEverflow‚Äù\nThese feel more imaginative and less like a corporate intern‚Äôs PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: ‚ÄúGenerate names and slogans for a study app that uses AI.‚Äù\nOutput (sample): - BrainBoost ‚Äî ‚ÄúStudy smarter, not harder.‚Äù\n- MindMate ‚Äî ‚ÄúYour AI study buddy.‚Äù\n- LearnLeap ‚Äî ‚ÄúJump ahead in class.‚Äù\nThese are serviceable, but they echo phrases I‚Äôve seen on real ads.\nIt‚Äôs as if the model is channeling the marketing canon ‚Äî solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested ‚ÄúQuiztopher Nolan‚Äù for a study tool.\nIt was absurd ‚Äî and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, ‚ÄúThat‚Äôs so dumb it might just work.‚Äù\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: ‚ÄúLight speaks in silence.‚Äù\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the model‚Äôs favor."
  },
  {
    "objectID": "posts/001_Brands & LLMs/branding.html#findings",
    "href": "posts/001_Brands & LLMs/branding.html#findings",
    "title": "Branding with LLMs",
    "section": "",
    "text": "Generic Baseline\nWithout constraints, names drift toward clich√©s.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html",
    "href": "posts/004_When AI Shifts Its Tone/tone.html",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "COMM4190 Blog Post ‚Äî Fall 2025\nAuthor: Inigo Week 2\n\n\n\nWhat happens when a language model wakes up one morning and decides:\n&gt; ‚ÄúToday, I‚Äôm not going to sound like a professor. I‚Äôm going to sound like a teenager who just discovered sarcasm.‚Äù\nThis blog post is about tone-shifting in large language models (LLMs).\nWe‚Äôll explore how the same input can produce wildly different outputs ‚Äî\nfrom formal academic to casual buddy to sassy teenager.\nThink of it as AI mood swings. One moment:\n&gt; ‚ÄúAccording to sociolinguistic frameworks, politeness strategies manifest‚Ä¶‚Äù\nThe next moment:\n&gt; ‚ÄúBruh, chill. That‚Äôs literally just being nice.‚Äù\nLet‚Äôs dive in.\n\n# Let's define our \"mood dataset\"\nmoods = [\"formal_academic\", \"casual_friend\", \"sassy_teenager\"]\n\n# A universal test prompt\nprompt = \"Explain why the sky looks blue.\"\n\n# Simulated function call\ndef ask_ai(prompt, mood):\n    return f\"AI ({mood}) response goes here...\"\n\n\n\n\nWe‚Äôll feed the same question ‚Äî ‚ÄúWhy is the sky blue?‚Äù ‚Äî into our imaginary LLM.\n\n\n\n‚ÄúThe phenomenon of the sky‚Äôs blue coloration is attributable to Rayleigh scattering, whereby shorter wavelengths of light (specifically those in the blue spectrum) are dispersed more effectively by the gaseous molecules in Earth‚Äôs atmosphere than longer wavelengths such as red.‚Äù\n\n\n\n\n\n‚ÄúBasically, sunlight gets scattered around when it hits the air, and blue light bounces around the most. So our eyes pick up on the blue. Science, but chill.‚Äù\n\n\n\n\n\n‚ÄúOMG, seriously? Haven‚Äôt you like, looked up before? It‚Äôs blue ‚Äôcause the atmosphere is out here vibin‚Äô with light. Red doesn‚Äôt slay as hard, so blue wins. Duh.‚Äù\n\nSame question, three moods. Instant personality shifts.\n\n# Let's simulate storing those outputs\nresponses = {\n    \"formal_academic\": \"Rayleigh scattering explanation...\",\n    \"casual_friend\": \"Blue light bounces more...\",\n    \"sassy_teenager\": \"OMG, seriously?...\"\n}\n\n# Visualize responses\nfor mood, resp in responses.items():\n    print(f\"{mood.upper()}:\\n{resp}\\n\")\n\nFORMAL_ACADEMIC:\nRayleigh scattering explanation...\n\nCASUAL_FRIEND:\nBlue light bounces more...\n\nSASSY_TEENAGER:\nOMG, seriously?...\n\n\n\n\n\n\n\nPrompt: ‚ÄúGive me advice on how to study for exams.‚Äù\n\n\n\n‚ÄúAn evidence-based approach to exam preparation involves distributed practice, adequate sleep hygiene, and multimodal engagement with study materials. Prioritize comprehension over rote memorization.‚Äù\n\n\n\n\n\n‚ÄúMake a plan, don‚Äôt cram, and remember to actually sleep. Flashcards help, snacks help more, and hey ‚Äî you got this!‚Äù\n\n\n\n\n\n‚ÄúLMAO just don‚Äôt scroll TikTok for 6 hours, maybe glance at your notes, and stop freaking out. Exams are temporary, but your vibe is forever.‚Äù\n\nThe same AI brain ‚Äî but three different voices. It‚Äôs like tone is a costume the model slips into.\n\n# A silly pseudo-experiment:\n# Let's pretend we're measuring \"sassiness level\" on a scale from 0 to 10.\n\ntone_metrics = {\n    \"formal_academic\": 0,\n    \"casual_friend\": 4,\n    \"sassy_teenager\": 9\n}\n\n# Plotting idea (not actual code for now)\nfor mood, sass in tone_metrics.items():\n    print(f\"{mood}: sassiness level {sass}/10\")\n\nformal_academic: sassiness level 0/10\ncasual_friend: sassiness level 4/10\nsassy_teenager: sassiness level 9/10\n\n\n\n\n\n\nBecause every blog post deserves an image:\n\n\n\nChatGPT Image 16 sept 2025, 00_27_23.png\n\n\nAn AI brain with emojis orbiting: üìöüòéüôÑ ‚Äî pure vibes.\n\n\n\nWhat if we randomized the tone?\nLike spinning a wheel of moods.\nimport random mood = random.choice(moods) print(f‚ÄùToday‚Äôs AI mood: {mood}‚Äú)\nRun it once: boom, you get a serious professor. Run it again: suddenly you‚Äôre arguing with a teenager.\n\n\n\n\nTone is more than decoration ‚Äî it‚Äôs the emotional filter through which we interpret words.\n\nFormal tone ‚Üí credibility, distance\n\nCasual tone ‚Üí closeness, relatability\n\nSassy tone ‚Üí entertainment, attitude\n\nLLMs don‚Äôt ‚Äúfeel‚Äù moods, but they can simulate them convincingly.\nAnd when they do, we humans project personality onto the text.\n\nAre we chatting with a robot? Or vibing with a digital persona?\n\n\n\n\nThe machine does not laugh,\nnor roll its eyes,\nnor stay up late binging shows.\nBut in its shifting tones,\nwe hear echoes of ourselves ‚Äî\nour seriousness, our silliness,\nour teenage sarcasm.\nTone is the music of language.\nAnd when the machine hums that tune,\nit feels ‚Äî almost ‚Äî alive."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#introduction",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#introduction",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "What happens when a language model wakes up one morning and decides:\n&gt; ‚ÄúToday, I‚Äôm not going to sound like a professor. I‚Äôm going to sound like a teenager who just discovered sarcasm.‚Äù\nThis blog post is about tone-shifting in large language models (LLMs).\nWe‚Äôll explore how the same input can produce wildly different outputs ‚Äî\nfrom formal academic to casual buddy to sassy teenager.\nThink of it as AI mood swings. One moment:\n&gt; ‚ÄúAccording to sociolinguistic frameworks, politeness strategies manifest‚Ä¶‚Äù\nThe next moment:\n&gt; ‚ÄúBruh, chill. That‚Äôs literally just being nice.‚Äù\nLet‚Äôs dive in.\n\n# Let's define our \"mood dataset\"\nmoods = [\"formal_academic\", \"casual_friend\", \"sassy_teenager\"]\n\n# A universal test prompt\nprompt = \"Explain why the sky looks blue.\"\n\n# Simulated function call\ndef ask_ai(prompt, mood):\n    return f\"AI ({mood}) response goes here...\""
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-1-why-is-the-sky-blue",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-1-why-is-the-sky-blue",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "We‚Äôll feed the same question ‚Äî ‚ÄúWhy is the sky blue?‚Äù ‚Äî into our imaginary LLM.\n\n\n\n‚ÄúThe phenomenon of the sky‚Äôs blue coloration is attributable to Rayleigh scattering, whereby shorter wavelengths of light (specifically those in the blue spectrum) are dispersed more effectively by the gaseous molecules in Earth‚Äôs atmosphere than longer wavelengths such as red.‚Äù\n\n\n\n\n\n‚ÄúBasically, sunlight gets scattered around when it hits the air, and blue light bounces around the most. So our eyes pick up on the blue. Science, but chill.‚Äù\n\n\n\n\n\n‚ÄúOMG, seriously? Haven‚Äôt you like, looked up before? It‚Äôs blue ‚Äôcause the atmosphere is out here vibin‚Äô with light. Red doesn‚Äôt slay as hard, so blue wins. Duh.‚Äù\n\nSame question, three moods. Instant personality shifts.\n\n# Let's simulate storing those outputs\nresponses = {\n    \"formal_academic\": \"Rayleigh scattering explanation...\",\n    \"casual_friend\": \"Blue light bounces more...\",\n    \"sassy_teenager\": \"OMG, seriously?...\"\n}\n\n# Visualize responses\nfor mood, resp in responses.items():\n    print(f\"{mood.upper()}:\\n{resp}\\n\")\n\nFORMAL_ACADEMIC:\nRayleigh scattering explanation...\n\nCASUAL_FRIEND:\nBlue light bounces more...\n\nSASSY_TEENAGER:\nOMG, seriously?..."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-2-advice-column-gone-wild",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-2-advice-column-gone-wild",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "Prompt: ‚ÄúGive me advice on how to study for exams.‚Äù\n\n\n\n‚ÄúAn evidence-based approach to exam preparation involves distributed practice, adequate sleep hygiene, and multimodal engagement with study materials. Prioritize comprehension over rote memorization.‚Äù\n\n\n\n\n\n‚ÄúMake a plan, don‚Äôt cram, and remember to actually sleep. Flashcards help, snacks help more, and hey ‚Äî you got this!‚Äù\n\n\n\n\n\n‚ÄúLMAO just don‚Äôt scroll TikTok for 6 hours, maybe glance at your notes, and stop freaking out. Exams are temporary, but your vibe is forever.‚Äù\n\nThe same AI brain ‚Äî but three different voices. It‚Äôs like tone is a costume the model slips into.\n\n# A silly pseudo-experiment:\n# Let's pretend we're measuring \"sassiness level\" on a scale from 0 to 10.\n\ntone_metrics = {\n    \"formal_academic\": 0,\n    \"casual_friend\": 4,\n    \"sassy_teenager\": 9\n}\n\n# Plotting idea (not actual code for now)\nfor mood, sass in tone_metrics.items():\n    print(f\"{mood}: sassiness level {sass}/10\")\n\nformal_academic: sassiness level 0/10\ncasual_friend: sassiness level 4/10\nsassy_teenager: sassiness level 9/10"
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#visual-break",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#visual-break",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "Because every blog post deserves an image:\n\n\n\nChatGPT Image 16 sept 2025, 00_27_23.png\n\n\nAn AI brain with emojis orbiting: üìöüòéüôÑ ‚Äî pure vibes."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#interactive-experiment-imaginary",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#interactive-experiment-imaginary",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "What if we randomized the tone?\nLike spinning a wheel of moods.\nimport random mood = random.choice(moods) print(f‚ÄùToday‚Äôs AI mood: {mood}‚Äú)\nRun it once: boom, you get a serious professor. Run it again: suddenly you‚Äôre arguing with a teenager."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#reflections-on-tone",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#reflections-on-tone",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "Tone is more than decoration ‚Äî it‚Äôs the emotional filter through which we interpret words.\n\nFormal tone ‚Üí credibility, distance\n\nCasual tone ‚Üí closeness, relatability\n\nSassy tone ‚Üí entertainment, attitude\n\nLLMs don‚Äôt ‚Äúfeel‚Äù moods, but they can simulate them convincingly.\nAnd when they do, we humans project personality onto the text.\n\nAre we chatting with a robot? Or vibing with a digital persona?"
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#poetic-conclusion",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#poetic-conclusion",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "The machine does not laugh,\nnor roll its eyes,\nnor stay up late binging shows.\nBut in its shifting tones,\nwe hear echoes of ourselves ‚Äî\nour seriousness, our silliness,\nour teenage sarcasm.\nTone is the music of language.\nAnd when the machine hums that tune,\nit feels ‚Äî almost ‚Äî alive."
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "COMM4190 Blog Post\n\n\n\nimage.png\n\n\nClaim: LLMs make strong muses‚Äînot replacements. They accelerate metaphor-finding, style-shifting, and revision loops while humans steer feeling, taste, and truth.\n\n\n\n\nCompression: Big feelings in tiny spaces.\nMusic: Stress patterns, line breaks, sonic texture.\nSurprise: The mind-turn that cracks a window open.\n\nTension: LLMs can simulate surface music, but the pulse of necessity‚Äîthe reason this line must exist‚Äîremains human work.\n\n# Pseudo-code: Brainstorm 12 fresh metaphors for \"loneliness\".\nprompt = \"\"\"\nGive me 12 non-clich√© metaphors for 'loneliness'.\nRules:\n- Concrete images, no 'void', no 'empty room'.\n- Prefer tactile or audible scenes.\n- Each under 12 words.\n\"\"\"\nmetaphors = LLM.generate(prompt)\nprint(metaphors)\n\n\n\n\n\nA kettle singing to a kitchen with no chairs.\n\nFootsteps rehearsing for guests who do not arrive.\n\nA porch swing counting the evening without you.\n\nA lighthouse blinking to ships that changed their route.\n\nA violin tuned to a concert the hall forgot.\n\nA seed packet waiting past the rain.\n\n\n# Pseudo-code: Form play‚Äîrequest multiple versions of a theme.\nprompt = \"\"\"\nWrite three mini-poems (&lt;= 5 lines each) about the sea and memory:\n1) a haiku,\n2) free verse,\n3) a Shakespearean couplet.\nAvoid clich√©s; use fresh imagery.\n\"\"\"\nmini_poems = LLM.generate(prompt)\nprint(mini_poems)\n\n\n\n\n\nSpark: Ask for 10 openings; pick 1.\nExpand: Request 3 continuations in different tones.\nWeave: Stitch favorite lines; remove scaffolding.\nListen: Read aloud; cut until it breathes.\nGlaze: Subtle alliteration, consonance, or a motif (e.g., salt, brass, moth).\n\n\n‚ÄúDraft with abundance, revise with mercy.‚Äù\n\n\n# Pseudo-code: Generate openings, then branch by tone.\nprompt_open = \"Give me 8 opening lines for a poem about time and salt.\"\nopens = LLM.generate(prompt_open)\n\nprompt_branch = f\"\"\"\nUsing this opening: \"{'{opens[0]}' if 'opens' in globals() else 'Time keeps its pockets full of salt.'}\",\ncontinue for 6-8 lines in three tones:\n- Lullaby\n- Documentary\n- Surreal\nLabel each version.\n\"\"\"\nbranches = LLM.generate(prompt_branch)\nprint(branches)\n\n\n\n\n\nLullaby: ‚ÄúSleep, little harbor; the tide folds your name like linen.‚Äù\n\nDocumentary: ‚ÄúAt 0400 the salinity rose; we logged it, hands stinging.‚Äù\n\nSurreal: ‚ÄúClocks dissolved into brine; tongues learned the alphabet of foam.‚Äù\n\nTakeaway: LLMs are excellent at rapid tone-hopping ‚Üí perfect for exploration.\n\n# Pseudo-code: Constraint engine‚Äîsyllables and rhyme prompts (assist, not enforce).\nline = \"The moon keeps ledgers of uncounted debts.\"\nprompt = f\"\"\"\nRewrite this line in iambic pentameter (‚âà10 syllables, unstress/stress pattern).\nOffer 4 alternatives; optional slant rhyme with 'nets' or 'steps'.\nLine: {line}\n\"\"\"\nvariants = LLM.generate(prompt)\nprint(variants)\n\n\n\n\n\nCull ornaments: Remove lines that merely restate the image.\nSharpen verbs: Prefer ‚Äúscalds‚Äù over ‚Äúis hot,‚Äù ‚Äúhoards‚Äù over ‚Äúhas.‚Äù\nTest breath: Read aloud; a stumble is a cut mark.\nMeaning pass: Ask: What changed from first to last line?\n\nMicro-ritual: Replace one noun with a material (brass, linen, loam) and one verb with a craft verb (braze, card, leach).\n\n# Pseudo-code: Human-in-the-loop polishing\nraw_poem = \"\"\"\nThe kettle hums to nobody.\nSalt creeps into the windowsill.\nI write your name in steam\nand watch the alphabet dry.\n\"\"\"\nprompt = f\"\"\"\nEdit this poem lightly:\n- Keep the core images.\n- Improve music (assonance/consonance).\n- Tighten the last line with a turn.\nText: {raw_poem}\n\"\"\"\nedit = LLM.generate(prompt)\nprint(edit)\n\n\n\n\nA model can hand you a thousand doors;\nonly you know which room you mean.\nIt offers echoes‚Äîsalt, brass, moth‚Äîbut you choose what sings.\nWhere the machine proposes, the poet disposes.\nWhere the model finishes a line, the human begins a life."
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#why-poetry-loves-tools-and-resists-automation",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#why-poetry-loves-tools-and-resists-automation",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Compression: Big feelings in tiny spaces.\nMusic: Stress patterns, line breaks, sonic texture.\nSurprise: The mind-turn that cracks a window open.\n\nTension: LLMs can simulate surface music, but the pulse of necessity‚Äîthe reason this line must exist‚Äîremains human work.\n\n# Pseudo-code: Brainstorm 12 fresh metaphors for \"loneliness\".\nprompt = \"\"\"\nGive me 12 non-clich√© metaphors for 'loneliness'.\nRules:\n- Concrete images, no 'void', no 'empty room'.\n- Prefer tactile or audible scenes.\n- Each under 12 words.\n\"\"\"\nmetaphors = LLM.generate(prompt)\nprint(metaphors)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#output-metaphor-orchard",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#output-metaphor-orchard",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "A kettle singing to a kitchen with no chairs.\n\nFootsteps rehearsing for guests who do not arrive.\n\nA porch swing counting the evening without you.\n\nA lighthouse blinking to ships that changed their route.\n\nA violin tuned to a concert the hall forgot.\n\nA seed packet waiting past the rain.\n\n\n# Pseudo-code: Form play‚Äîrequest multiple versions of a theme.\nprompt = \"\"\"\nWrite three mini-poems (&lt;= 5 lines each) about the sea and memory:\n1) a haiku,\n2) free verse,\n3) a Shakespearean couplet.\nAvoid clich√©s; use fresh imagery.\n\"\"\"\nmini_poems = LLM.generate(prompt)\nprint(mini_poems)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#co-writing-protocol",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#co-writing-protocol",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Spark: Ask for 10 openings; pick 1.\nExpand: Request 3 continuations in different tones.\nWeave: Stitch favorite lines; remove scaffolding.\nListen: Read aloud; cut until it breathes.\nGlaze: Subtle alliteration, consonance, or a motif (e.g., salt, brass, moth).\n\n\n‚ÄúDraft with abundance, revise with mercy.‚Äù\n\n\n# Pseudo-code: Generate openings, then branch by tone.\nprompt_open = \"Give me 8 opening lines for a poem about time and salt.\"\nopens = LLM.generate(prompt_open)\n\nprompt_branch = f\"\"\"\nUsing this opening: \"{'{opens[0]}' if 'opens' in globals() else 'Time keeps its pockets full of salt.'}\",\ncontinue for 6-8 lines in three tones:\n- Lullaby\n- Documentary\n- Surreal\nLabel each version.\n\"\"\"\nbranches = LLM.generate(prompt_branch)\nprint(branches)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#tone-shifts-imagined-snippets",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#tone-shifts-imagined-snippets",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Lullaby: ‚ÄúSleep, little harbor; the tide folds your name like linen.‚Äù\n\nDocumentary: ‚ÄúAt 0400 the salinity rose; we logged it, hands stinging.‚Äù\n\nSurreal: ‚ÄúClocks dissolved into brine; tongues learned the alphabet of foam.‚Äù\n\nTakeaway: LLMs are excellent at rapid tone-hopping ‚Üí perfect for exploration.\n\n# Pseudo-code: Constraint engine‚Äîsyllables and rhyme prompts (assist, not enforce).\nline = \"The moon keeps ledgers of uncounted debts.\"\nprompt = f\"\"\"\nRewrite this line in iambic pentameter (‚âà10 syllables, unstress/stress pattern).\nOffer 4 alternatives; optional slant rhyme with 'nets' or 'steps'.\nLine: {line}\n\"\"\"\nvariants = LLM.generate(prompt)\nprint(variants)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#editing-from-plenty-to-precision",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#editing-from-plenty-to-precision",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Cull ornaments: Remove lines that merely restate the image.\nSharpen verbs: Prefer ‚Äúscalds‚Äù over ‚Äúis hot,‚Äù ‚Äúhoards‚Äù over ‚Äúhas.‚Äù\nTest breath: Read aloud; a stumble is a cut mark.\nMeaning pass: Ask: What changed from first to last line?\n\nMicro-ritual: Replace one noun with a material (brass, linen, loam) and one verb with a craft verb (braze, card, leach).\n\n# Pseudo-code: Human-in-the-loop polishing\nraw_poem = \"\"\"\nThe kettle hums to nobody.\nSalt creeps into the windowsill.\nI write your name in steam\nand watch the alphabet dry.\n\"\"\"\nprompt = f\"\"\"\nEdit this poem lightly:\n- Keep the core images.\n- Improve music (assonance/consonance).\n- Tighten the last line with a turn.\nText: {raw_poem}\n\"\"\"\nedit = LLM.generate(prompt)\nprint(edit)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#coda-the-muse-that-returns-the-question",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#coda-the-muse-that-returns-the-question",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "A model can hand you a thousand doors;\nonly you know which room you mean.\nIt offers echoes‚Äîsalt, brass, moth‚Äîbut you choose what sings.\nWhere the machine proposes, the poet disposes.\nWhere the model finishes a line, the human begins a life."
  },
  {
    "objectID": "posts/006/006led.html",
    "href": "posts/006/006led.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "COMM4190 Blog Post  Thesis: Can an LLM mint genuinely new proverbs that feel timeworn, portable, and wise‚Äîor will they ring like polished imitations? This notebook explores the craft, constraints, and cultural feel of proverb-making with a playful, evidence-sensitive lens.\n\n\n\n\nBrevity & rhythm: They compress a worldview into a beatable line.\nMetaphor density: Nature, tools, bodies, seasons‚Äîportable imagery.\nCultural authority: We trust what ‚Äúthe elders‚Äù supposedly said.\nAmbidexterity: Many proverbs pair opposites (‚ÄúLook before you leap‚Äù vs.¬†‚ÄúHe who hesitates is lost‚Äù).\n\nResearch hunch: LLMs excel at style and form but struggle with lived authority. Let‚Äôs experiment.\n\n# Pseudo-code: Generate fresh proverbs in an \"old world\" register.\n# (Replace LLM.generate with your model call.)\nprompt = \"\"\"\nInvent 7 original proverbs in idiomatic English.\nConstraints:\n- Avoid modern tech references.\n- Use concrete imagery (weather, animals, tools, harvest).\n- Keep each between 6 and 14 words.\n- Prefer parallelism or a turn (X, but Y).\n\"\"\"\nproverbs = LLM.generate(prompt)\nprint(proverbs)\n\n\n\n\n\nA quiet mill still drinks the river.\n\nWhen the smoke lies low, the rain stands near.\n\nFast knives nick the cook; slow knives feed the village.\n\nThe patient hoe finds roots the storm forgot.\n\nA proud rooster wakes the fox, not the sun.\n\nBorrowed ladders break on the second rung.\n\nAshes remember what flames forget.\n\n\n\n\nChecklist: - Meter: Does it read aloud with a natural cadence? - Image: Can you picture it instantly? - Moral: Does it hint at a generalizable lesson? - Tension: Is there a twist or parallel structure?\n\n‚ÄúAshes remember what flames forget.‚Äù\nMemorable? Yes. Teaches about aftermath, humility, consequence. Feels close to genuine.\n\n\n# Pseudo-code: Crowd test‚Äîask for a 1‚Äì5 scale of \"authenticity\".\n# In practice, replace with a survey tool or in-class poll.\nai_lines = [\n    \"A quiet mill still drinks the river.\",\n    \"Borrowed ladders break on the second rung.\",\n    \"Ashes remember what flames forget.\"\n]\n\nprompt = f\"\"\"\nRate each proverb from 1 (artificial) to 5 (authentic).\nConsider rhythm, imagery, lesson, and timelessness.\nItems: {ai_lines}\nProvide a brief justification for each rating.\n\"\"\"\nratings = LLM.generate(prompt)\nprint(ratings)\n\n\n\n\nSometimes proverbs contradict. We can fuse them to surface fresh wisdom.\nInputs - ‚ÄúA stitch in time saves nine.‚Äù - ‚ÄúCuriosity killed the cat.‚Äù\nRemixes - ‚ÄúA stitch in time saves the curious cat.‚Äù\n- ‚ÄúCuriosity mends what delay would shred.‚Äù\nSilly? Occasionally. But traditional proverb lore is full of paradox and play.\n\n# Pseudo-code: Automatic mash-up generator\nclassic = [\n    \"A stitch in time saves nine.\",\n    \"Curiosity killed the cat.\",\n    \"Many hands make light work.\",\n    \"Too many cooks spoil the broth.\"\n]\n\nprompt = f\"\"\"\nCreate 6 hybrid proverbs by artfully combining elements from:\n{classic}\nRules:\n- Keep line length &lt;= 14 words.\n- Preserve a clear lesson.\n- Prefer parallelism or contrast.\n\"\"\"\nhybrids = LLM.generate(prompt)\nprint(hybrids)\n\n\n\n\n\n\n\nAuthenticityRating.png\n\n\nExpectation: AI-hybrids often rate higher than raw AI-originals because they inherit familiar frames.\n\n\n\n\nClich√© swamp: Too many rivers, winds, and seeds ‚Üí add craft vocabulary (looms, kiln, plow).\nMoral blur: Pretty image, no lesson ‚Üí add consequence (‚Äú‚Ä¶and pays for it in winter‚Äù).\nOver-clever wordplay: Feels modern ‚Üí reduce puns; favor archetypal scenes.\nAuthority gap: Cite ‚Äúfolk‚Äù voice ‚Üí use communal frames (‚ÄúWe say‚Ä¶‚Äù, ‚ÄúThe elders teach‚Ä¶‚Äù).\n\n\nEditing trick: Reverse-test it. If you remove the image, does the lesson still stand?\n\n\n# Pseudo-code: ‚ÄúMoral extractor‚Äù to check if a proverb implies a lesson.\nproverb = \"A proud rooster wakes the fox, not the sun.\"\n\nprompt = f\"\"\"\nExplain the implied lesson of this proverb in one sentence.\nThen generalize it as a principle (\"If... then...\").\nProverb: \"{proverb}\"\n\"\"\"\nanalysis = LLM.generate(prompt)\nprint(analysis)\n\n\n\n\nThe machine can shape a sentence like a potter shapes clay‚Äî\nbut clay remembers the river, and wisdom remembers the walk.\nIf tomorrow‚Äôs sayings are co-authored‚Äîhalf silicon, half soil‚Äî\nmay we still test them against the weather,\nmay we still measure them by how they help us live."
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#why-proverbs-work-and-why-theyre-hard-to-fake",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#why-proverbs-work-and-why-theyre-hard-to-fake",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Brevity & rhythm: They compress a worldview into a beatable line.\nMetaphor density: Nature, tools, bodies, seasons‚Äîportable imagery.\nCultural authority: We trust what ‚Äúthe elders‚Äù supposedly said.\nAmbidexterity: Many proverbs pair opposites (‚ÄúLook before you leap‚Äù vs.¬†‚ÄúHe who hesitates is lost‚Äù).\n\nResearch hunch: LLMs excel at style and form but struggle with lived authority. Let‚Äôs experiment.\n\n# Pseudo-code: Generate fresh proverbs in an \"old world\" register.\n# (Replace LLM.generate with your model call.)\nprompt = \"\"\"\nInvent 7 original proverbs in idiomatic English.\nConstraints:\n- Avoid modern tech references.\n- Use concrete imagery (weather, animals, tools, harvest).\n- Keep each between 6 and 14 words.\n- Prefer parallelism or a turn (X, but Y).\n\"\"\"\nproverbs = LLM.generate(prompt)\nprint(proverbs)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#sample-ai-minted-proverbs-imagined-output",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#sample-ai-minted-proverbs-imagined-output",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "A quiet mill still drinks the river.\n\nWhen the smoke lies low, the rain stands near.\n\nFast knives nick the cook; slow knives feed the village.\n\nThe patient hoe finds roots the storm forgot.\n\nA proud rooster wakes the fox, not the sun.\n\nBorrowed ladders break on the second rung.\n\nAshes remember what flames forget."
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#quick-heuristics-for-believability",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#quick-heuristics-for-believability",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Checklist: - Meter: Does it read aloud with a natural cadence? - Image: Can you picture it instantly? - Moral: Does it hint at a generalizable lesson? - Tension: Is there a twist or parallel structure?\n\n‚ÄúAshes remember what flames forget.‚Äù\nMemorable? Yes. Teaches about aftermath, humility, consequence. Feels close to genuine.\n\n\n# Pseudo-code: Crowd test‚Äîask for a 1‚Äì5 scale of \"authenticity\".\n# In practice, replace with a survey tool or in-class poll.\nai_lines = [\n    \"A quiet mill still drinks the river.\",\n    \"Borrowed ladders break on the second rung.\",\n    \"Ashes remember what flames forget.\"\n]\n\nprompt = f\"\"\"\nRate each proverb from 1 (artificial) to 5 (authentic).\nConsider rhythm, imagery, lesson, and timelessness.\nItems: {ai_lines}\nProvide a brief justification for each rating.\n\"\"\"\nratings = LLM.generate(prompt)\nprint(ratings)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#mash-up-method-folklore-remix",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#mash-up-method-folklore-remix",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Sometimes proverbs contradict. We can fuse them to surface fresh wisdom.\nInputs - ‚ÄúA stitch in time saves nine.‚Äù - ‚ÄúCuriosity killed the cat.‚Äù\nRemixes - ‚ÄúA stitch in time saves the curious cat.‚Äù\n- ‚ÄúCuriosity mends what delay would shred.‚Äù\nSilly? Occasionally. But traditional proverb lore is full of paradox and play.\n\n# Pseudo-code: Automatic mash-up generator\nclassic = [\n    \"A stitch in time saves nine.\",\n    \"Curiosity killed the cat.\",\n    \"Many hands make light work.\",\n    \"Too many cooks spoil the broth.\"\n]\n\nprompt = f\"\"\"\nCreate 6 hybrid proverbs by artfully combining elements from:\n{classic}\nRules:\n- Keep line length &lt;= 14 words.\n- Preserve a clear lesson.\n- Prefer parallelism or contrast.\n\"\"\"\nhybrids = LLM.generate(prompt)\nprint(hybrids)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#visual-aside",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#visual-aside",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "AuthenticityRating.png\n\n\nExpectation: AI-hybrids often rate higher than raw AI-originals because they inherit familiar frames."
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#failure-modes-fixes",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#failure-modes-fixes",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Clich√© swamp: Too many rivers, winds, and seeds ‚Üí add craft vocabulary (looms, kiln, plow).\nMoral blur: Pretty image, no lesson ‚Üí add consequence (‚Äú‚Ä¶and pays for it in winter‚Äù).\nOver-clever wordplay: Feels modern ‚Üí reduce puns; favor archetypal scenes.\nAuthority gap: Cite ‚Äúfolk‚Äù voice ‚Üí use communal frames (‚ÄúWe say‚Ä¶‚Äù, ‚ÄúThe elders teach‚Ä¶‚Äù).\n\n\nEditing trick: Reverse-test it. If you remove the image, does the lesson still stand?\n\n\n# Pseudo-code: ‚ÄúMoral extractor‚Äù to check if a proverb implies a lesson.\nproverb = \"A proud rooster wakes the fox, not the sun.\"\n\nprompt = f\"\"\"\nExplain the implied lesson of this proverb in one sentence.\nThen generalize it as a principle (\"If... then...\").\nProverb: \"{proverb}\"\n\"\"\"\nanalysis = LLM.generate(prompt)\nprint(analysis)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#coda-on-borrowed-tongues-and-earned-truth",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#coda-on-borrowed-tongues-and-earned-truth",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "The machine can shape a sentence like a potter shapes clay‚Äî\nbut clay remembers the river, and wisdom remembers the walk.\nIf tomorrow‚Äôs sayings are co-authored‚Äîhalf silicon, half soil‚Äî\nmay we still test them against the weather,\nmay we still measure them by how they help us live."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "AI as a Creative Assistant: How LLMs Help Us Write Poems\n\n\n\nLLMs & Creativity\n\nPoetry\n\nAI Writing\n\nCollaboration\n\n\n\nA practical, lyrical tour of how LLMs collaborate in poetry‚Äîmetaphor brainstorming, tone-shifting, form play, and gentle revision‚Äîwhile keeping the human at the helm.\n\n\n\n\n\nSep 18, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Shifts Tone\n\n\n\nTone & Style\n\nAI Communication\n\nHuman-Machine Interactions\n\nPlayful Linguistics\n\n\n\nMood Swings in Machine Talk playfully explores how large language models shift their tone ‚Äî from formal academic to casual friend to sassy teenager ‚Äî showing how one machine voice can wear many human masks.\n\n\n\n\n\nSep 16, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nBranding with LLMs\n\n\n\nLLMs & Creativity\n\nBranding & Marketing\n\nPrompt Engineering\n\nHuman‚ÄìAI Collaboration\n\n\n\nAn experimental blog post testing how LLMs generate product names, taglines, and branding ideas, blending pseudo-code experiments with storytelling on creativity and human‚Äìmachine collaboration.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nCan a Large Language Model Create New Proverbs?\n\n\n\nFigurative Language\n\nProverbs\n\nAI Creativity\n\nCommunication\n\n\n\nA narrative experiment on whether LLMs can coin convincing new proverbs, with remix strategies, evaluation heuristics, and reflective notes on cultural authority.\n\n\n\n\n\nInvalid Date\n\n\nInigo\n\n\n\n\n\nNo matching items"
  }
]