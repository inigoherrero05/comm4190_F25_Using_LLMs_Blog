[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001_Can LLMs recognize and use idioms naturally?/Can LLMs recognize and use idioms naturally?.html",
    "href": "posts/001_Can LLMs recognize and use idioms naturally?/Can LLMs recognize and use idioms naturally?.html",
    "title": "Branding with LLMs",
    "section": "",
    "text": "COMM4190 · Prompt/Response & Readings Blog\nWeek Y — “Branding & Creativity”\n\n\n\n\nimage.png\n\n\n\n\n\nI still remember sitting in a café brainstorming names for a startup with two friends.\nOne suggested “CloudJar,” another “NeoNest,” and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity — or would it just spit out clichés?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n---------------------------------------------------------------------------\nOpenAIError                               Traceback (most recent call last)\nCell In[6], line 2\n      1 from openai import OpenAI\n----&gt; 2 client = OpenAI()\n      4 idioms = [\"kick the bucket\", \"spill the beans\", \"hit the sack\"]\n      5 scenarios = [\"literal context\", \"figurative context\"]\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/_client.py:110, in OpenAI.__init__(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\n    108     api_key = os.environ.get(\"OPENAI_API_KEY\")\n    109 if api_key is None:\n--&gt; 110     raise OpenAIError(\n    111         \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n    112     )\n    113 self.api_key = api_key\n    115 if organization is None:\n\nOpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n\n\n\n\nPrompt: “Generate 5 product names and a tagline.”\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Nature’s Sip\nTagline: “Drink clean. Live green.”\nNot bad. But some names feel generic — a reminder that LLMs lean on frequency patterns.\nStill, “PureSip” hits the ear nicely, the kind of name you’d actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clichés like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - “Raincatcher”\n- “Blue Whisper”\n- “Everflow”\nThese feel more imaginative and less like a corporate intern’s PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: “Generate names and slogans for a study app that uses AI.”\nOutput (sample): - BrainBoost — “Study smarter, not harder.”\n- MindMate — “Your AI study buddy.”\n- LearnLeap — “Jump ahead in class.”\nThese are serviceable, but they echo phrases I’ve seen on real ads.\nIt’s as if the model is channeling the marketing canon — solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested “Quiztopher Nolan” for a study tool.\nIt was absurd — and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, “That’s so dumb it might just work.”\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: “Light speaks in silence.”\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the model’s favor.\n\n\n\n\n\nGeneric Baseline\nWithout constraints, names drift toward clichés.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/001_Can LLMs recognize and use idioms naturally?/Can LLMs recognize and use idioms naturally?.html#the-branding-experiment",
    "href": "posts/001_Can LLMs recognize and use idioms naturally?/Can LLMs recognize and use idioms naturally?.html#the-branding-experiment",
    "title": "Branding with LLMs",
    "section": "",
    "text": "I still remember sitting in a café brainstorming names for a startup with two friends.\nOne suggested “CloudJar,” another “NeoNest,” and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity — or would it just spit out clichés?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n---------------------------------------------------------------------------\nOpenAIError                               Traceback (most recent call last)\nCell In[6], line 2\n      1 from openai import OpenAI\n----&gt; 2 client = OpenAI()\n      4 idioms = [\"kick the bucket\", \"spill the beans\", \"hit the sack\"]\n      5 scenarios = [\"literal context\", \"figurative context\"]\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/_client.py:110, in OpenAI.__init__(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\n    108     api_key = os.environ.get(\"OPENAI_API_KEY\")\n    109 if api_key is None:\n--&gt; 110     raise OpenAIError(\n    111         \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n    112     )\n    113 self.api_key = api_key\n    115 if organization is None:\n\nOpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n\n\n\n\nPrompt: “Generate 5 product names and a tagline.”\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Nature’s Sip\nTagline: “Drink clean. Live green.”\nNot bad. But some names feel generic — a reminder that LLMs lean on frequency patterns.\nStill, “PureSip” hits the ear nicely, the kind of name you’d actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clichés like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - “Raincatcher”\n- “Blue Whisper”\n- “Everflow”\nThese feel more imaginative and less like a corporate intern’s PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: “Generate names and slogans for a study app that uses AI.”\nOutput (sample): - BrainBoost — “Study smarter, not harder.”\n- MindMate — “Your AI study buddy.”\n- LearnLeap — “Jump ahead in class.”\nThese are serviceable, but they echo phrases I’ve seen on real ads.\nIt’s as if the model is channeling the marketing canon — solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested “Quiztopher Nolan” for a study tool.\nIt was absurd — and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, “That’s so dumb it might just work.”\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: “Light speaks in silence.”\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the model’s favor."
  },
  {
    "objectID": "posts/001_Can LLMs recognize and use idioms naturally?/Can LLMs recognize and use idioms naturally?.html#findings",
    "href": "posts/001_Can LLMs recognize and use idioms naturally?/Can LLMs recognize and use idioms naturally?.html#findings",
    "title": "Branding with LLMs",
    "section": "",
    "text": "Generic Baseline\nWithout constraints, names drift toward clichés.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Branding with LLMs\n\n\n\nLLMs & Creativity\n\nBranding & Marketing\n\nPrompt Engineering\n\nHuman–AI Collaboration\n\n\n\nAn experimental blog post testing how LLMs generate product names, taglines, and branding ideas, blending pseudo-code experiments with storytelling on creativity and human–machine collaboration.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  }
]