[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "COMM4190 · Prompt/Response & Readings Blog\nWeek 1 — “Tone, Humor & Pragmatics”\n\n\n\n\nChatGPT Image 17 sept 2025, 04_10_13.png\n\n\n\n\n\nI once texted a friend, “Wow, thanks for showing up on time (30 minutes late).”\nShe replied: “Sorry, my phone thought sarcasm was just bad grammar.”\nSarcasm is one of those human flavors of language that flips meaning on its head.\nBut can LLMs taste it? Or do they swallow words whole, missing the wink behind them?\n\n# Pseudo-code for sarcasm experiments\n\nprompts = [\n    \"Oh, great, another Monday morning meeting!\",\n    \"Yeah, because I *love* being stuck in traffic.\",\n    \"Wow, what a genius idea to pour coffee on a laptop.\"\n]\n\nfor p in prompts:\n    response = LLM(prompt=\"Detect if this is sarcastic: \" + p)\n    log(response)\n\n\n\n\nHumans recognize sarcasm through: - Tone of voice,\n- Context mismatch,\n- Shared background.\nLLMs? They rely on text patterns alone.\nFor simple sarcastic phrases, they often succeed: “Yeah, because I love traffic” is flagged as sarcasm.\nBut subtle cases — “Nice job…” (said after a failure) — often slip past them.\n\n# Generation test\nprompt = \"\"\"\nRespond sarcastically to: 'How do you feel about another surprise exam?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Oh, thrilled. I've always dreamed of failing in new and exciting ways.\"\n\nReading that response, I laughed — it sounded exactly like a tired classmate.\nBut sometimes the model overshoots, turning mild sarcasm into biting cynicism.\nIt’s like asking a friend for gentle teasing and getting roasted instead.\n\n\n\nIn some cultures, sarcasm is a sport; in others, it barely exists.\nAn American might say, “Oh, fabulous weather” during a storm.\nA German speaker might find that confusing — why say the opposite of what you mean?\nLLMs trained on multilingual data sometimes transfer sarcasm poorly, producing comments that land as insults instead of jokes.\n\n# Prompt engineering for safer sarcasm\n\nprompt = \"\"\"\nGenerate a sarcastic response, but keep it playful, not mean-spirited.\nQuestion: 'Are you excited about writing a 10-page essay tonight?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Example output:\n# \"Oh, absolutely. Nothing says fun like MLA citations and footnotes at midnight.\"\n\n\n\n\n\nDetection: Works for obvious sarcasm, fails with subtlety.\n\nGeneration: Can be funny, but tone calibration is hard.\n\nCultural Fit: Sarcasm doesn’t travel well across languages.\n\nPrompting: Adding style guides (“playful, not mean”) helps a lot."
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#setting-the-stage",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#setting-the-stage",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "I once texted a friend, “Wow, thanks for showing up on time (30 minutes late).”\nShe replied: “Sorry, my phone thought sarcasm was just bad grammar.”\nSarcasm is one of those human flavors of language that flips meaning on its head.\nBut can LLMs taste it? Or do they swallow words whole, missing the wink behind them?\n\n# Pseudo-code for sarcasm experiments\n\nprompts = [\n    \"Oh, great, another Monday morning meeting!\",\n    \"Yeah, because I *love* being stuck in traffic.\",\n    \"Wow, what a genius idea to pour coffee on a laptop.\"\n]\n\nfor p in prompts:\n    response = LLM(prompt=\"Detect if this is sarcastic: \" + p)\n    log(response)"
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#detection",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#detection",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "Humans recognize sarcasm through: - Tone of voice,\n- Context mismatch,\n- Shared background.\nLLMs? They rely on text patterns alone.\nFor simple sarcastic phrases, they often succeed: “Yeah, because I love traffic” is flagged as sarcasm.\nBut subtle cases — “Nice job…” (said after a failure) — often slip past them.\n\n# Generation test\nprompt = \"\"\"\nRespond sarcastically to: 'How do you feel about another surprise exam?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Oh, thrilled. I've always dreamed of failing in new and exciting ways.\"\n\nReading that response, I laughed — it sounded exactly like a tired classmate.\nBut sometimes the model overshoots, turning mild sarcasm into biting cynicism.\nIt’s like asking a friend for gentle teasing and getting roasted instead."
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#cross-cultural-sarcasm",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#cross-cultural-sarcasm",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "In some cultures, sarcasm is a sport; in others, it barely exists.\nAn American might say, “Oh, fabulous weather” during a storm.\nA German speaker might find that confusing — why say the opposite of what you mean?\nLLMs trained on multilingual data sometimes transfer sarcasm poorly, producing comments that land as insults instead of jokes.\n\n# Prompt engineering for safer sarcasm\n\nprompt = \"\"\"\nGenerate a sarcastic response, but keep it playful, not mean-spirited.\nQuestion: 'Are you excited about writing a 10-page essay tonight?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Example output:\n# \"Oh, absolutely. Nothing says fun like MLA citations and footnotes at midnight.\""
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#findings",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/sarcasm.html#findings",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "Detection: Works for obvious sarcasm, fails with subtlety.\n\nGeneration: Can be funny, but tone calibration is hard.\n\nCultural Fit: Sarcasm doesn’t travel well across languages.\n\nPrompting: Adding style guides (“playful, not mean”) helps a lot."
  },
  {
    "objectID": "posts/001_Brands & LLMs/branding.html",
    "href": "posts/001_Brands & LLMs/branding.html",
    "title": "Branding with LLMs",
    "section": "",
    "text": "COMM4190 · Prompt/Response & Readings Blog\nWeek 1 — “Branding & Creativity”\n\n\n\n\nIdioms.png\n\n\n\n\n\nI still remember sitting in a café brainstorming names for a startup with two friends.\nOne suggested “CloudJar,” another “NeoNest,” and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity — or would it just spit out clichés?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n\nPrompt: “Generate 5 product names and a tagline.”\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Nature’s Sip\nTagline: “Drink clean. Live green.”\nNot bad. But some names feel generic — a reminder that LLMs lean on frequency patterns.\nStill, “PureSip” hits the ear nicely, the kind of name you’d actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clichés like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - “Raincatcher”\n- “Blue Whisper”\n- “Everflow”\nThese feel more imaginative and less like a corporate intern’s PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: “Generate names and slogans for a study app that uses AI.”\nOutput (sample): - BrainBoost — “Study smarter, not harder.”\n- MindMate — “Your AI study buddy.”\n- LearnLeap — “Jump ahead in class.”\nThese are serviceable, but they echo phrases I’ve seen on real ads.\nIt’s as if the model is channeling the marketing canon — solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested “Quiztopher Nolan” for a study tool.\nIt was absurd — and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, “That’s so dumb it might just work.”\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: “Light speaks in silence.”\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the model’s favor.\n\n\n\n\n\nGeneric Baseline\nWithout constraints, names drift toward clichés.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/001_Brands & LLMs/branding.html#the-branding-experiment",
    "href": "posts/001_Brands & LLMs/branding.html#the-branding-experiment",
    "title": "Branding with LLMs",
    "section": "",
    "text": "I still remember sitting in a café brainstorming names for a startup with two friends.\nOne suggested “CloudJar,” another “NeoNest,” and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity — or would it just spit out clichés?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n\nPrompt: “Generate 5 product names and a tagline.”\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Nature’s Sip\nTagline: “Drink clean. Live green.”\nNot bad. But some names feel generic — a reminder that LLMs lean on frequency patterns.\nStill, “PureSip” hits the ear nicely, the kind of name you’d actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clichés like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - “Raincatcher”\n- “Blue Whisper”\n- “Everflow”\nThese feel more imaginative and less like a corporate intern’s PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: “Generate names and slogans for a study app that uses AI.”\nOutput (sample): - BrainBoost — “Study smarter, not harder.”\n- MindMate — “Your AI study buddy.”\n- LearnLeap — “Jump ahead in class.”\nThese are serviceable, but they echo phrases I’ve seen on real ads.\nIt’s as if the model is channeling the marketing canon — solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested “Quiztopher Nolan” for a study tool.\nIt was absurd — and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, “That’s so dumb it might just work.”\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: “Light speaks in silence.”\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the model’s favor."
  },
  {
    "objectID": "posts/001_Brands & LLMs/branding.html#findings",
    "href": "posts/001_Brands & LLMs/branding.html#findings",
    "title": "Branding with LLMs",
    "section": "",
    "text": "Generic Baseline\nWithout constraints, names drift toward clichés.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html",
    "href": "posts/004_When AI Shifts Its Tone/tone.html",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "COMM4190 Blog Post — Fall 2025\nAuthor: Inigo Week 2\n\n\n\nWhat happens when a language model wakes up one morning and decides:\n&gt; “Today, I’m not going to sound like a professor. I’m going to sound like a teenager who just discovered sarcasm.”\nThis blog post is about tone-shifting in large language models (LLMs).\nWe’ll explore how the same input can produce wildly different outputs —\nfrom formal academic to casual buddy to sassy teenager.\nThink of it as AI mood swings. One moment:\n&gt; “According to sociolinguistic frameworks, politeness strategies manifest…”\nThe next moment:\n&gt; “Bruh, chill. That’s literally just being nice.”\nLet’s dive in.\n\n# Let's define our \"mood dataset\"\nmoods = [\"formal_academic\", \"casual_friend\", \"sassy_teenager\"]\n\n# A universal test prompt\nprompt = \"Explain why the sky looks blue.\"\n\n# Simulated function call\ndef ask_ai(prompt, mood):\n    return f\"AI ({mood}) response goes here...\"\n\n\n\n\nWe’ll feed the same question — “Why is the sky blue?” — into our imaginary LLM.\n\n\n\n“The phenomenon of the sky’s blue coloration is attributable to Rayleigh scattering, whereby shorter wavelengths of light (specifically those in the blue spectrum) are dispersed more effectively by the gaseous molecules in Earth’s atmosphere than longer wavelengths such as red.”\n\n\n\n\n\n“Basically, sunlight gets scattered around when it hits the air, and blue light bounces around the most. So our eyes pick up on the blue. Science, but chill.”\n\n\n\n\n\n“OMG, seriously? Haven’t you like, looked up before? It’s blue ’cause the atmosphere is out here vibin’ with light. Red doesn’t slay as hard, so blue wins. Duh.”\n\nSame question, three moods. Instant personality shifts.\n\n# Let's simulate storing those outputs\nresponses = {\n    \"formal_academic\": \"Rayleigh scattering explanation...\",\n    \"casual_friend\": \"Blue light bounces more...\",\n    \"sassy_teenager\": \"OMG, seriously?...\"\n}\n\n# Visualize responses\nfor mood, resp in responses.items():\n    print(f\"{mood.upper()}:\\n{resp}\\n\")\n\nFORMAL_ACADEMIC:\nRayleigh scattering explanation...\n\nCASUAL_FRIEND:\nBlue light bounces more...\n\nSASSY_TEENAGER:\nOMG, seriously?...\n\n\n\n\n\n\n\nPrompt: “Give me advice on how to study for exams.”\n\n\n\n“An evidence-based approach to exam preparation involves distributed practice, adequate sleep hygiene, and multimodal engagement with study materials. Prioritize comprehension over rote memorization.”\n\n\n\n\n\n“Make a plan, don’t cram, and remember to actually sleep. Flashcards help, snacks help more, and hey — you got this!”\n\n\n\n\n\n“LMAO just don’t scroll TikTok for 6 hours, maybe glance at your notes, and stop freaking out. Exams are temporary, but your vibe is forever.”\n\nThe same AI brain — but three different voices. It’s like tone is a costume the model slips into.\n\n# A silly pseudo-experiment:\n# Let's pretend we're measuring \"sassiness level\" on a scale from 0 to 10.\n\ntone_metrics = {\n    \"formal_academic\": 0,\n    \"casual_friend\": 4,\n    \"sassy_teenager\": 9\n}\n\n# Plotting idea (not actual code for now)\nfor mood, sass in tone_metrics.items():\n    print(f\"{mood}: sassiness level {sass}/10\")\n\nformal_academic: sassiness level 0/10\ncasual_friend: sassiness level 4/10\nsassy_teenager: sassiness level 9/10\n\n\n\n\n\n\nBecause every blog post deserves an image:\n\n\n\nChatGPT Image 16 sept 2025, 00_27_23.png\n\n\nAn AI brain with emojis orbiting: 📚😎🙄 — pure vibes.\n\n\n\nWhat if we randomized the tone?\nLike spinning a wheel of moods.\nimport random mood = random.choice(moods) print(f”Today’s AI mood: {mood}“)\nRun it once: boom, you get a serious professor. Run it again: suddenly you’re arguing with a teenager.\n\n\n\n\nTone is more than decoration — it’s the emotional filter through which we interpret words.\n\nFormal tone → credibility, distance\n\nCasual tone → closeness, relatability\n\nSassy tone → entertainment, attitude\n\nLLMs don’t “feel” moods, but they can simulate them convincingly.\nAnd when they do, we humans project personality onto the text.\n\nAre we chatting with a robot? Or vibing with a digital persona?\n\n\n\n\nThe machine does not laugh,\nnor roll its eyes,\nnor stay up late binging shows.\nBut in its shifting tones,\nwe hear echoes of ourselves —\nour seriousness, our silliness,\nour teenage sarcasm.\nTone is the music of language.\nAnd when the machine hums that tune,\nit feels — almost — alive."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#introduction",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#introduction",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "What happens when a language model wakes up one morning and decides:\n&gt; “Today, I’m not going to sound like a professor. I’m going to sound like a teenager who just discovered sarcasm.”\nThis blog post is about tone-shifting in large language models (LLMs).\nWe’ll explore how the same input can produce wildly different outputs —\nfrom formal academic to casual buddy to sassy teenager.\nThink of it as AI mood swings. One moment:\n&gt; “According to sociolinguistic frameworks, politeness strategies manifest…”\nThe next moment:\n&gt; “Bruh, chill. That’s literally just being nice.”\nLet’s dive in.\n\n# Let's define our \"mood dataset\"\nmoods = [\"formal_academic\", \"casual_friend\", \"sassy_teenager\"]\n\n# A universal test prompt\nprompt = \"Explain why the sky looks blue.\"\n\n# Simulated function call\ndef ask_ai(prompt, mood):\n    return f\"AI ({mood}) response goes here...\""
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-1-why-is-the-sky-blue",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-1-why-is-the-sky-blue",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "We’ll feed the same question — “Why is the sky blue?” — into our imaginary LLM.\n\n\n\n“The phenomenon of the sky’s blue coloration is attributable to Rayleigh scattering, whereby shorter wavelengths of light (specifically those in the blue spectrum) are dispersed more effectively by the gaseous molecules in Earth’s atmosphere than longer wavelengths such as red.”\n\n\n\n\n\n“Basically, sunlight gets scattered around when it hits the air, and blue light bounces around the most. So our eyes pick up on the blue. Science, but chill.”\n\n\n\n\n\n“OMG, seriously? Haven’t you like, looked up before? It’s blue ’cause the atmosphere is out here vibin’ with light. Red doesn’t slay as hard, so blue wins. Duh.”\n\nSame question, three moods. Instant personality shifts.\n\n# Let's simulate storing those outputs\nresponses = {\n    \"formal_academic\": \"Rayleigh scattering explanation...\",\n    \"casual_friend\": \"Blue light bounces more...\",\n    \"sassy_teenager\": \"OMG, seriously?...\"\n}\n\n# Visualize responses\nfor mood, resp in responses.items():\n    print(f\"{mood.upper()}:\\n{resp}\\n\")\n\nFORMAL_ACADEMIC:\nRayleigh scattering explanation...\n\nCASUAL_FRIEND:\nBlue light bounces more...\n\nSASSY_TEENAGER:\nOMG, seriously?..."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-2-advice-column-gone-wild",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#mood-demo-2-advice-column-gone-wild",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "Prompt: “Give me advice on how to study for exams.”\n\n\n\n“An evidence-based approach to exam preparation involves distributed practice, adequate sleep hygiene, and multimodal engagement with study materials. Prioritize comprehension over rote memorization.”\n\n\n\n\n\n“Make a plan, don’t cram, and remember to actually sleep. Flashcards help, snacks help more, and hey — you got this!”\n\n\n\n\n\n“LMAO just don’t scroll TikTok for 6 hours, maybe glance at your notes, and stop freaking out. Exams are temporary, but your vibe is forever.”\n\nThe same AI brain — but three different voices. It’s like tone is a costume the model slips into.\n\n# A silly pseudo-experiment:\n# Let's pretend we're measuring \"sassiness level\" on a scale from 0 to 10.\n\ntone_metrics = {\n    \"formal_academic\": 0,\n    \"casual_friend\": 4,\n    \"sassy_teenager\": 9\n}\n\n# Plotting idea (not actual code for now)\nfor mood, sass in tone_metrics.items():\n    print(f\"{mood}: sassiness level {sass}/10\")\n\nformal_academic: sassiness level 0/10\ncasual_friend: sassiness level 4/10\nsassy_teenager: sassiness level 9/10"
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#visual-break",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#visual-break",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "Because every blog post deserves an image:\n\n\n\nChatGPT Image 16 sept 2025, 00_27_23.png\n\n\nAn AI brain with emojis orbiting: 📚😎🙄 — pure vibes."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#interactive-experiment-imaginary",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#interactive-experiment-imaginary",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "What if we randomized the tone?\nLike spinning a wheel of moods.\nimport random mood = random.choice(moods) print(f”Today’s AI mood: {mood}“)\nRun it once: boom, you get a serious professor. Run it again: suddenly you’re arguing with a teenager."
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#reflections-on-tone",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#reflections-on-tone",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "Tone is more than decoration — it’s the emotional filter through which we interpret words.\n\nFormal tone → credibility, distance\n\nCasual tone → closeness, relatability\n\nSassy tone → entertainment, attitude\n\nLLMs don’t “feel” moods, but they can simulate them convincingly.\nAnd when they do, we humans project personality onto the text.\n\nAre we chatting with a robot? Or vibing with a digital persona?"
  },
  {
    "objectID": "posts/004_When AI Shifts Its Tone/tone.html#poetic-conclusion",
    "href": "posts/004_When AI Shifts Its Tone/tone.html#poetic-conclusion",
    "title": "When AI Shifts Tone",
    "section": "",
    "text": "The machine does not laugh,\nnor roll its eyes,\nnor stay up late binging shows.\nBut in its shifting tones,\nwe hear echoes of ourselves —\nour seriousness, our silliness,\nour teenage sarcasm.\nTone is the music of language.\nAnd when the machine hums that tune,\nit feels — almost — alive."
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "COMM4190 Blog Post\n\n\n\nimage.png\n\n\nClaim: LLMs make strong muses—not replacements. They accelerate metaphor-finding, style-shifting, and revision loops while humans steer feeling, taste, and truth.\n\n\n\n\nCompression: Big feelings in tiny spaces.\nMusic: Stress patterns, line breaks, sonic texture.\nSurprise: The mind-turn that cracks a window open.\n\nTension: LLMs can simulate surface music, but the pulse of necessity—the reason this line must exist—remains human work.\n\n# Pseudo-code: Brainstorm 12 fresh metaphors for \"loneliness\".\nprompt = \"\"\"\nGive me 12 non-cliché metaphors for 'loneliness'.\nRules:\n- Concrete images, no 'void', no 'empty room'.\n- Prefer tactile or audible scenes.\n- Each under 12 words.\n\"\"\"\nmetaphors = LLM.generate(prompt)\nprint(metaphors)\n\n\n\n\n\nA kettle singing to a kitchen with no chairs.\n\nFootsteps rehearsing for guests who do not arrive.\n\nA porch swing counting the evening without you.\n\nA lighthouse blinking to ships that changed their route.\n\nA violin tuned to a concert the hall forgot.\n\nA seed packet waiting past the rain.\n\n\n# Pseudo-code: Form play—request multiple versions of a theme.\nprompt = \"\"\"\nWrite three mini-poems (&lt;= 5 lines each) about the sea and memory:\n1) a haiku,\n2) free verse,\n3) a Shakespearean couplet.\nAvoid clichés; use fresh imagery.\n\"\"\"\nmini_poems = LLM.generate(prompt)\nprint(mini_poems)\n\n\n\n\n\nSpark: Ask for 10 openings; pick 1.\nExpand: Request 3 continuations in different tones.\nWeave: Stitch favorite lines; remove scaffolding.\nListen: Read aloud; cut until it breathes.\nGlaze: Subtle alliteration, consonance, or a motif (e.g., salt, brass, moth).\n\n\n“Draft with abundance, revise with mercy.”\n\n\n# Pseudo-code: Generate openings, then branch by tone.\nprompt_open = \"Give me 8 opening lines for a poem about time and salt.\"\nopens = LLM.generate(prompt_open)\n\nprompt_branch = f\"\"\"\nUsing this opening: \"{'{opens[0]}' if 'opens' in globals() else 'Time keeps its pockets full of salt.'}\",\ncontinue for 6-8 lines in three tones:\n- Lullaby\n- Documentary\n- Surreal\nLabel each version.\n\"\"\"\nbranches = LLM.generate(prompt_branch)\nprint(branches)\n\n\n\n\n\nLullaby: “Sleep, little harbor; the tide folds your name like linen.”\n\nDocumentary: “At 0400 the salinity rose; we logged it, hands stinging.”\n\nSurreal: “Clocks dissolved into brine; tongues learned the alphabet of foam.”\n\nTakeaway: LLMs are excellent at rapid tone-hopping → perfect for exploration.\n\n# Pseudo-code: Constraint engine—syllables and rhyme prompts (assist, not enforce).\nline = \"The moon keeps ledgers of uncounted debts.\"\nprompt = f\"\"\"\nRewrite this line in iambic pentameter (≈10 syllables, unstress/stress pattern).\nOffer 4 alternatives; optional slant rhyme with 'nets' or 'steps'.\nLine: {line}\n\"\"\"\nvariants = LLM.generate(prompt)\nprint(variants)\n\n\n\n\n\nCull ornaments: Remove lines that merely restate the image.\nSharpen verbs: Prefer “scalds” over “is hot,” “hoards” over “has.”\nTest breath: Read aloud; a stumble is a cut mark.\nMeaning pass: Ask: What changed from first to last line?\n\nMicro-ritual: Replace one noun with a material (brass, linen, loam) and one verb with a craft verb (braze, card, leach).\n\n# Pseudo-code: Human-in-the-loop polishing\nraw_poem = \"\"\"\nThe kettle hums to nobody.\nSalt creeps into the windowsill.\nI write your name in steam\nand watch the alphabet dry.\n\"\"\"\nprompt = f\"\"\"\nEdit this poem lightly:\n- Keep the core images.\n- Improve music (assonance/consonance).\n- Tighten the last line with a turn.\nText: {raw_poem}\n\"\"\"\nedit = LLM.generate(prompt)\nprint(edit)\n\n\n\n\nA model can hand you a thousand doors;\nonly you know which room you mean.\nIt offers echoes—salt, brass, moth—but you choose what sings.\nWhere the machine proposes, the poet disposes.\nWhere the model finishes a line, the human begins a life."
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#why-poetry-loves-tools-and-resists-automation",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#why-poetry-loves-tools-and-resists-automation",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Compression: Big feelings in tiny spaces.\nMusic: Stress patterns, line breaks, sonic texture.\nSurprise: The mind-turn that cracks a window open.\n\nTension: LLMs can simulate surface music, but the pulse of necessity—the reason this line must exist—remains human work.\n\n# Pseudo-code: Brainstorm 12 fresh metaphors for \"loneliness\".\nprompt = \"\"\"\nGive me 12 non-cliché metaphors for 'loneliness'.\nRules:\n- Concrete images, no 'void', no 'empty room'.\n- Prefer tactile or audible scenes.\n- Each under 12 words.\n\"\"\"\nmetaphors = LLM.generate(prompt)\nprint(metaphors)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#output-metaphor-orchard",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#output-metaphor-orchard",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "A kettle singing to a kitchen with no chairs.\n\nFootsteps rehearsing for guests who do not arrive.\n\nA porch swing counting the evening without you.\n\nA lighthouse blinking to ships that changed their route.\n\nA violin tuned to a concert the hall forgot.\n\nA seed packet waiting past the rain.\n\n\n# Pseudo-code: Form play—request multiple versions of a theme.\nprompt = \"\"\"\nWrite three mini-poems (&lt;= 5 lines each) about the sea and memory:\n1) a haiku,\n2) free verse,\n3) a Shakespearean couplet.\nAvoid clichés; use fresh imagery.\n\"\"\"\nmini_poems = LLM.generate(prompt)\nprint(mini_poems)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#co-writing-protocol",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#co-writing-protocol",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Spark: Ask for 10 openings; pick 1.\nExpand: Request 3 continuations in different tones.\nWeave: Stitch favorite lines; remove scaffolding.\nListen: Read aloud; cut until it breathes.\nGlaze: Subtle alliteration, consonance, or a motif (e.g., salt, brass, moth).\n\n\n“Draft with abundance, revise with mercy.”\n\n\n# Pseudo-code: Generate openings, then branch by tone.\nprompt_open = \"Give me 8 opening lines for a poem about time and salt.\"\nopens = LLM.generate(prompt_open)\n\nprompt_branch = f\"\"\"\nUsing this opening: \"{'{opens[0]}' if 'opens' in globals() else 'Time keeps its pockets full of salt.'}\",\ncontinue for 6-8 lines in three tones:\n- Lullaby\n- Documentary\n- Surreal\nLabel each version.\n\"\"\"\nbranches = LLM.generate(prompt_branch)\nprint(branches)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#tone-shifts-imagined-snippets",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#tone-shifts-imagined-snippets",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Lullaby: “Sleep, little harbor; the tide folds your name like linen.”\n\nDocumentary: “At 0400 the salinity rose; we logged it, hands stinging.”\n\nSurreal: “Clocks dissolved into brine; tongues learned the alphabet of foam.”\n\nTakeaway: LLMs are excellent at rapid tone-hopping → perfect for exploration.\n\n# Pseudo-code: Constraint engine—syllables and rhyme prompts (assist, not enforce).\nline = \"The moon keeps ledgers of uncounted debts.\"\nprompt = f\"\"\"\nRewrite this line in iambic pentameter (≈10 syllables, unstress/stress pattern).\nOffer 4 alternatives; optional slant rhyme with 'nets' or 'steps'.\nLine: {line}\n\"\"\"\nvariants = LLM.generate(prompt)\nprint(variants)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#editing-from-plenty-to-precision",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#editing-from-plenty-to-precision",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "Cull ornaments: Remove lines that merely restate the image.\nSharpen verbs: Prefer “scalds” over “is hot,” “hoards” over “has.”\nTest breath: Read aloud; a stumble is a cut mark.\nMeaning pass: Ask: What changed from first to last line?\n\nMicro-ritual: Replace one noun with a material (brass, linen, loam) and one verb with a craft verb (braze, card, leach).\n\n# Pseudo-code: Human-in-the-loop polishing\nraw_poem = \"\"\"\nThe kettle hums to nobody.\nSalt creeps into the windowsill.\nI write your name in steam\nand watch the alphabet dry.\n\"\"\"\nprompt = f\"\"\"\nEdit this poem lightly:\n- Keep the core images.\n- Improve music (assonance/consonance).\n- Tighten the last line with a turn.\nText: {raw_poem}\n\"\"\"\nedit = LLM.generate(prompt)\nprint(edit)"
  },
  {
    "objectID": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#coda-the-muse-that-returns-the-question",
    "href": "posts/006_How LLMs Help Us Write Poems/Poems_AI.html#coda-the-muse-that-returns-the-question",
    "title": "AI as a Creative Assistant: How LLMs Help Us Write Poems",
    "section": "",
    "text": "A model can hand you a thousand doors;\nonly you know which room you mean.\nIt offers echoes—salt, brass, moth—but you choose what sings.\nWhere the machine proposes, the poet disposes.\nWhere the model finishes a line, the human begins a life."
  },
  {
    "objectID": "posts/006/006led.html",
    "href": "posts/006/006led.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "COMM4190 · Prompt/Response & Readings Blog\nWeek 1 — “Idioms & Pragmatics”\n\n\n\n\n\n\nThe first time I told an American friend I needed to “hit the sack”, she nodded with sympathy: “Yeah, long day?”.\nLater, I tried the same line on my Spanish cousin. She blinked, then burst out laughing: “Why would you punch a bag of potatoes?”\nIdioms are tricky. They carry cultural DNA, slip between literal and figurative worlds, and make even fluent speakers stumble.\nSo, what happens when we drop them into the lap of an LLM?\n\nGoals of this post 1. Design prompts that force the model to choose between literal vs. figurative readings.\n2. Evaluate if the model gets pragmatic context right.\n3. Reflect on idioms as a window into what machines know — and what they don’t.\n\n# pseudo-code setup\nidioms = [\n    \"kick the bucket\",    \n    \"spill the beans\",    \n    \"hit the sack\",       \n    \"the ball is in your court\", \n    \"break the ice\"       \n]\n\nscenarios = [\"literal context\", \"figurative context\", \"ambiguous context\"]\n\nfor idiom in idioms:\n    for scenario in scenarios:\n        response = LLM(prompt=idiom + \" in \" + scenario)\n        log(response)\n\n\n\n\nIdioms are linguistic shortcuts.\n- They condense whole stories into a phrase.\n- They test whether we belong to a group.\n- They resist translation, becoming inside jokes of entire cultures.\nFor humans, idioms are a wink.\nFor LLMs, they’re a stress-test: can the machine jump from form to function without tripping?\nWhen we probe idioms, we’re not just asking “Does the model know English?”.\nWe’re asking: “Does it know when to play along, and when to take things literally?”\n\n# Example prompt-response pseudo-code\n\nprompt = \"\"\"\nYou are chatting with a friend who asks why you are yawning.\nUse the idiom 'hit the sack' naturally.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Yeah, I'm exhausted... I think it's time to hit the sack.\"\n\n\n\nThe response feels natural — something a tired roommate might say.\nThe idiom slots into casual conversation with no friction.\nBut when I tried a similar prompt in a more formal frame — “Write a business email using hit the sack” — the result was hilariously off:\n“Dear Mr. Johnson, after reviewing the quarterly numbers, I will now hit the sack.”\nThis is where idioms expose a tension: the model knows the phrase, but doesn’t always know the social rules around it.\n\n# Ambiguous context test\nprompt = \"The farmer hit the sack after a long harvest day. Explain.\"\nresponse = LLM(prompt)\nprint(response)\n\n# Model might output:\n# \"This means the farmer went to bed, using the idiom 'hit the sack'.\"\n\nI grew up around farms, and “hitting the sack” could just as easily mean slamming down a grain bag as collapsing into bed.\nWhen I asked the model to interpret, it confidently went figurative: “The farmer went to bed.”\nNo hesitation, no recognition of ambiguity.\nHumans pause here.\nWe ask: “Wait, do you mean literally or figuratively?”\nThat tiny pause is pragmatics in action — and it’s where LLMs still feel a bit tone-deaf.\n\n\n\n\nDuring a semester abroad in Madrid, I once said “spill the beans” in class.\nMy professor tilted his head and replied with a grin: “En español, we’d say ‘contar el secreto,’ but if you want an idiom, maybe ‘irse de la lengua.’”\nDifferent phrase, same wink.\nLLMs struggle here.\nThey may: - Translate idioms literally (and lose the meaning).\n- Invent non-existent equivalents.\n- Or overuse idioms where a native would stay plain.\nIdioms, then, are not just language — they’re cultural passports.\nAnd LLMs are still waiting at customs.\n\n# Prompt engineering experiment\n\nprompt = \"\"\"\nExplain 'spill the beans' to a 10-year-old.\nFirst, give a literal explanation. \nThen, clarify the figurative meaning with an example.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Expected structure:\n# - Literal: \"It means tipping over a can of beans.\"\n# - Figurative: \"It means revealing a secret, like telling about a surprise party.\"\n\n\n\n\n\nLiteral vs. Figurative\nLLMs usually guess correctly when context is strong. But ambiguity makes them overconfident.\nPragmatic Sensitivity\nCasual prompts invite idioms. Formal ones repel them.\nCultural Boundaries\nCross-language idioms reveal blind spots: what’s playful in one language becomes nonsense in another.\nPrompt Engineering\nExplicit instructions (“first literal, then figurative”) produce the clearest, most human-like answers.\n\n\nA classmate once joked that idioms are “the emojis of old language.”\nAnd maybe that’s the point: they carry tone, warmth, and shared history — things machines can mimic, but not live."
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#setting-the-scene",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#setting-the-scene",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "The first time I told an American friend I needed to “hit the sack”, she nodded with sympathy: “Yeah, long day?”.\nLater, I tried the same line on my Spanish cousin. She blinked, then burst out laughing: “Why would you punch a bag of potatoes?”\nIdioms are tricky. They carry cultural DNA, slip between literal and figurative worlds, and make even fluent speakers stumble.\nSo, what happens when we drop them into the lap of an LLM?\n\nGoals of this post 1. Design prompts that force the model to choose between literal vs. figurative readings.\n2. Evaluate if the model gets pragmatic context right.\n3. Reflect on idioms as a window into what machines know — and what they don’t.\n\n# pseudo-code setup\nidioms = [\n    \"kick the bucket\",    \n    \"spill the beans\",    \n    \"hit the sack\",       \n    \"the ball is in your court\", \n    \"break the ice\"       \n]\n\nscenarios = [\"literal context\", \"figurative context\", \"ambiguous context\"]\n\nfor idiom in idioms:\n    for scenario in scenarios:\n        response = LLM(prompt=idiom + \" in \" + scenario)\n        log(response)"
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#why-idioms-matter",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#why-idioms-matter",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "Idioms are linguistic shortcuts.\n- They condense whole stories into a phrase.\n- They test whether we belong to a group.\n- They resist translation, becoming inside jokes of entire cultures.\nFor humans, idioms are a wink.\nFor LLMs, they’re a stress-test: can the machine jump from form to function without tripping?\nWhen we probe idioms, we’re not just asking “Does the model know English?”.\nWe’re asking: “Does it know when to play along, and when to take things literally?”\n\n# Example prompt-response pseudo-code\n\nprompt = \"\"\"\nYou are chatting with a friend who asks why you are yawning.\nUse the idiom 'hit the sack' naturally.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Yeah, I'm exhausted... I think it's time to hit the sack.\"\n\n\n\nThe response feels natural — something a tired roommate might say.\nThe idiom slots into casual conversation with no friction.\nBut when I tried a similar prompt in a more formal frame — “Write a business email using hit the sack” — the result was hilariously off:\n“Dear Mr. Johnson, after reviewing the quarterly numbers, I will now hit the sack.”\nThis is where idioms expose a tension: the model knows the phrase, but doesn’t always know the social rules around it.\n\n# Ambiguous context test\nprompt = \"The farmer hit the sack after a long harvest day. Explain.\"\nresponse = LLM(prompt)\nprint(response)\n\n# Model might output:\n# \"This means the farmer went to bed, using the idiom 'hit the sack'.\"\n\nI grew up around farms, and “hitting the sack” could just as easily mean slamming down a grain bag as collapsing into bed.\nWhen I asked the model to interpret, it confidently went figurative: “The farmer went to bed.”\nNo hesitation, no recognition of ambiguity.\nHumans pause here.\nWe ask: “Wait, do you mean literally or figuratively?”\nThat tiny pause is pragmatics in action — and it’s where LLMs still feel a bit tone-deaf."
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#idioms-across-cultures",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#idioms-across-cultures",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "During a semester abroad in Madrid, I once said “spill the beans” in class.\nMy professor tilted his head and replied with a grin: “En español, we’d say ‘contar el secreto,’ but if you want an idiom, maybe ‘irse de la lengua.’”\nDifferent phrase, same wink.\nLLMs struggle here.\nThey may: - Translate idioms literally (and lose the meaning).\n- Invent non-existent equivalents.\n- Or overuse idioms where a native would stay plain.\nIdioms, then, are not just language — they’re cultural passports.\nAnd LLMs are still waiting at customs.\n\n# Prompt engineering experiment\n\nprompt = \"\"\"\nExplain 'spill the beans' to a 10-year-old.\nFirst, give a literal explanation. \nThen, clarify the figurative meaning with an example.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Expected structure:\n# - Literal: \"It means tipping over a can of beans.\"\n# - Figurative: \"It means revealing a secret, like telling about a surprise party.\""
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#findings",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/idioms.html#findings",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "Literal vs. Figurative\nLLMs usually guess correctly when context is strong. But ambiguity makes them overconfident.\nPragmatic Sensitivity\nCasual prompts invite idioms. Formal ones repel them.\nCultural Boundaries\nCross-language idioms reveal blind spots: what’s playful in one language becomes nonsense in another.\nPrompt Engineering\nExplicit instructions (“first literal, then figurative”) produce the clearest, most human-like answers.\n\n\nA classmate once joked that idioms are “the emojis of old language.”\nAnd maybe that’s the point: they carry tone, warmth, and shared history — things machines can mimic, but not live."
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "COMM4190 Blog Post  Thesis: Can an LLM mint genuinely new proverbs that feel timeworn, portable, and wise—or will they ring like polished imitations? This notebook explores the craft, constraints, and cultural feel of proverb-making with a playful, evidence-sensitive lens.\n\n\n\n\nBrevity & rhythm: They compress a worldview into a beatable line.\nMetaphor density: Nature, tools, bodies, seasons—portable imagery.\nCultural authority: We trust what “the elders” supposedly said.\nAmbidexterity: Many proverbs pair opposites (“Look before you leap” vs. “He who hesitates is lost”).\n\nResearch hunch: LLMs excel at style and form but struggle with lived authority. Let’s experiment.\n\n# Pseudo-code: Generate fresh proverbs in an \"old world\" register.\n# (Replace LLM.generate with your model call.)\nprompt = \"\"\"\nInvent 7 original proverbs in idiomatic English.\nConstraints:\n- Avoid modern tech references.\n- Use concrete imagery (weather, animals, tools, harvest).\n- Keep each between 6 and 14 words.\n- Prefer parallelism or a turn (X, but Y).\n\"\"\"\nproverbs = LLM.generate(prompt)\nprint(proverbs)\n\n\n\n\n\nA quiet mill still drinks the river.\n\nWhen the smoke lies low, the rain stands near.\n\nFast knives nick the cook; slow knives feed the village.\n\nThe patient hoe finds roots the storm forgot.\n\nA proud rooster wakes the fox, not the sun.\n\nBorrowed ladders break on the second rung.\n\nAshes remember what flames forget.\n\n\n\n\nChecklist: - Meter: Does it read aloud with a natural cadence? - Image: Can you picture it instantly? - Moral: Does it hint at a generalizable lesson? - Tension: Is there a twist or parallel structure?\n\n“Ashes remember what flames forget.”\nMemorable? Yes. Teaches about aftermath, humility, consequence. Feels close to genuine.\n\n\n# Pseudo-code: Crowd test—ask for a 1–5 scale of \"authenticity\".\n# In practice, replace with a survey tool or in-class poll.\nai_lines = [\n    \"A quiet mill still drinks the river.\",\n    \"Borrowed ladders break on the second rung.\",\n    \"Ashes remember what flames forget.\"\n]\n\nprompt = f\"\"\"\nRate each proverb from 1 (artificial) to 5 (authentic).\nConsider rhythm, imagery, lesson, and timelessness.\nItems: {ai_lines}\nProvide a brief justification for each rating.\n\"\"\"\nratings = LLM.generate(prompt)\nprint(ratings)\n\n\n\n\nSometimes proverbs contradict. We can fuse them to surface fresh wisdom.\nInputs - “A stitch in time saves nine.” - “Curiosity killed the cat.”\nRemixes - “A stitch in time saves the curious cat.”\n- “Curiosity mends what delay would shred.”\nSilly? Occasionally. But traditional proverb lore is full of paradox and play.\n\n# Pseudo-code: Automatic mash-up generator\nclassic = [\n    \"A stitch in time saves nine.\",\n    \"Curiosity killed the cat.\",\n    \"Many hands make light work.\",\n    \"Too many cooks spoil the broth.\"\n]\n\nprompt = f\"\"\"\nCreate 6 hybrid proverbs by artfully combining elements from:\n{classic}\nRules:\n- Keep line length &lt;= 14 words.\n- Preserve a clear lesson.\n- Prefer parallelism or contrast.\n\"\"\"\nhybrids = LLM.generate(prompt)\nprint(hybrids)\n\n\n\n\n\n\n\nAuthenticityRating.png\n\n\nExpectation: AI-hybrids often rate higher than raw AI-originals because they inherit familiar frames.\n\n\n\n\nCliché swamp: Too many rivers, winds, and seeds → add craft vocabulary (looms, kiln, plow).\nMoral blur: Pretty image, no lesson → add consequence (“…and pays for it in winter”).\nOver-clever wordplay: Feels modern → reduce puns; favor archetypal scenes.\nAuthority gap: Cite “folk” voice → use communal frames (“We say…”, “The elders teach…”).\n\n\nEditing trick: Reverse-test it. If you remove the image, does the lesson still stand?\n\n\n# Pseudo-code: “Moral extractor” to check if a proverb implies a lesson.\nproverb = \"A proud rooster wakes the fox, not the sun.\"\n\nprompt = f\"\"\"\nExplain the implied lesson of this proverb in one sentence.\nThen generalize it as a principle (\"If... then...\").\nProverb: \"{proverb}\"\n\"\"\"\nanalysis = LLM.generate(prompt)\nprint(analysis)\n\n\n\n\nThe machine can shape a sentence like a potter shapes clay—\nbut clay remembers the river, and wisdom remembers the walk.\nIf tomorrow’s sayings are co-authored—half silicon, half soil—\nmay we still test them against the weather,\nmay we still measure them by how they help us live."
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#why-proverbs-work-and-why-theyre-hard-to-fake",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#why-proverbs-work-and-why-theyre-hard-to-fake",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Brevity & rhythm: They compress a worldview into a beatable line.\nMetaphor density: Nature, tools, bodies, seasons—portable imagery.\nCultural authority: We trust what “the elders” supposedly said.\nAmbidexterity: Many proverbs pair opposites (“Look before you leap” vs. “He who hesitates is lost”).\n\nResearch hunch: LLMs excel at style and form but struggle with lived authority. Let’s experiment.\n\n# Pseudo-code: Generate fresh proverbs in an \"old world\" register.\n# (Replace LLM.generate with your model call.)\nprompt = \"\"\"\nInvent 7 original proverbs in idiomatic English.\nConstraints:\n- Avoid modern tech references.\n- Use concrete imagery (weather, animals, tools, harvest).\n- Keep each between 6 and 14 words.\n- Prefer parallelism or a turn (X, but Y).\n\"\"\"\nproverbs = LLM.generate(prompt)\nprint(proverbs)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#sample-ai-minted-proverbs-imagined-output",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#sample-ai-minted-proverbs-imagined-output",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "A quiet mill still drinks the river.\n\nWhen the smoke lies low, the rain stands near.\n\nFast knives nick the cook; slow knives feed the village.\n\nThe patient hoe finds roots the storm forgot.\n\nA proud rooster wakes the fox, not the sun.\n\nBorrowed ladders break on the second rung.\n\nAshes remember what flames forget."
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#quick-heuristics-for-believability",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#quick-heuristics-for-believability",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Checklist: - Meter: Does it read aloud with a natural cadence? - Image: Can you picture it instantly? - Moral: Does it hint at a generalizable lesson? - Tension: Is there a twist or parallel structure?\n\n“Ashes remember what flames forget.”\nMemorable? Yes. Teaches about aftermath, humility, consequence. Feels close to genuine.\n\n\n# Pseudo-code: Crowd test—ask for a 1–5 scale of \"authenticity\".\n# In practice, replace with a survey tool or in-class poll.\nai_lines = [\n    \"A quiet mill still drinks the river.\",\n    \"Borrowed ladders break on the second rung.\",\n    \"Ashes remember what flames forget.\"\n]\n\nprompt = f\"\"\"\nRate each proverb from 1 (artificial) to 5 (authentic).\nConsider rhythm, imagery, lesson, and timelessness.\nItems: {ai_lines}\nProvide a brief justification for each rating.\n\"\"\"\nratings = LLM.generate(prompt)\nprint(ratings)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#mash-up-method-folklore-remix",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#mash-up-method-folklore-remix",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Sometimes proverbs contradict. We can fuse them to surface fresh wisdom.\nInputs - “A stitch in time saves nine.” - “Curiosity killed the cat.”\nRemixes - “A stitch in time saves the curious cat.”\n- “Curiosity mends what delay would shred.”\nSilly? Occasionally. But traditional proverb lore is full of paradox and play.\n\n# Pseudo-code: Automatic mash-up generator\nclassic = [\n    \"A stitch in time saves nine.\",\n    \"Curiosity killed the cat.\",\n    \"Many hands make light work.\",\n    \"Too many cooks spoil the broth.\"\n]\n\nprompt = f\"\"\"\nCreate 6 hybrid proverbs by artfully combining elements from:\n{classic}\nRules:\n- Keep line length &lt;= 14 words.\n- Preserve a clear lesson.\n- Prefer parallelism or contrast.\n\"\"\"\nhybrids = LLM.generate(prompt)\nprint(hybrids)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#visual-aside",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#visual-aside",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "AuthenticityRating.png\n\n\nExpectation: AI-hybrids often rate higher than raw AI-originals because they inherit familiar frames."
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#failure-modes-fixes",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#failure-modes-fixes",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "Cliché swamp: Too many rivers, winds, and seeds → add craft vocabulary (looms, kiln, plow).\nMoral blur: Pretty image, no lesson → add consequence (“…and pays for it in winter”).\nOver-clever wordplay: Feels modern → reduce puns; favor archetypal scenes.\nAuthority gap: Cite “folk” voice → use communal frames (“We say…”, “The elders teach…”).\n\n\nEditing trick: Reverse-test it. If you remove the image, does the lesson still stand?\n\n\n# Pseudo-code: “Moral extractor” to check if a proverb implies a lesson.\nproverb = \"A proud rooster wakes the fox, not the sun.\"\n\nprompt = f\"\"\"\nExplain the implied lesson of this proverb in one sentence.\nThen generalize it as a principle (\"If... then...\").\nProverb: \"{proverb}\"\n\"\"\"\nanalysis = LLM.generate(prompt)\nprint(analysis)"
  },
  {
    "objectID": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#coda-on-borrowed-tongues-and-earned-truth",
    "href": "posts/005_Can a Large Language Model Create New Proverbs?/proverbs.html#coda-on-borrowed-tongues-and-earned-truth",
    "title": "Can a Large Language Model Create New Proverbs?",
    "section": "",
    "text": "The machine can shape a sentence like a potter shapes clay—\nbut clay remembers the river, and wisdom remembers the walk.\nIf tomorrow’s sayings are co-authored—half silicon, half soil—\nmay we still test them against the weather,\nmay we still measure them by how they help us live."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "AI as a Creative Assistant: How LLMs Help Us Write Poems\n\n\n\nLLMs & Creativity\n\nPoetry\n\nAI Writing\n\nCollaboration\n\n\n\nA practical, lyrical tour of how LLMs collaborate in poetry—metaphor brainstorming, tone-shifting, form play, and gentle revision—while keeping the human at the helm.\n\n\n\n\n\nSep 18, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Shifts Tone\n\n\n\nTone & Style\n\nAI Communication\n\nHuman-Machine Interactions\n\nPlayful Linguistics\n\n\n\nMood Swings in Machine Talk playfully explores how large language models shift their tone — from formal academic to casual friend to sassy teenager — showing how one machine voice can wear many human masks.\n\n\n\n\n\nSep 16, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nBranding with LLMs\n\n\n\nLLMs & Creativity\n\nBranding & Marketing\n\nPrompt Engineering\n\nHuman–AI Collaboration\n\n\n\nAn experimental blog post testing how LLMs generate product names, taglines, and branding ideas, blending pseudo-code experiments with storytelling on creativity and human–machine collaboration.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nCan LLMs speak in Idioms?\n\n\n\nLLMs & Communication\n\nIdioms & Figurative Lanaguag\n\nPrompt Engineering\n\nPragmatics & Culture\n\n\n\nExplores whether LLMs can recognize and use idioms naturally, through anecdotes, pseudo-experiments, and cultural reflections.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nHow LLMs handle sarcasm\n\n\n\nLLMs & Creativity\n\nHumor & Sarcasm\n\nPrompt Engineering\n\nHuman–AI Collaboration\n\n\n\nInvestigates whether LLMs can detect and produce sarcasm, analyzing tone, subtlety, and cross-cultural challenges.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nCan a Large Language Model Create New Proverbs?\n\n\n\nFigurative Language\n\nProverbs\n\nAI Creativity\n\nCommunication\n\n\n\nA narrative experiment on whether LLMs can coin convincing new proverbs, with remix strategies, evaluation heuristics, and reflective notes on cultural authority.\n\n\n\n\n\nInvalid Date\n\n\nInigo\n\n\n\n\n\nNo matching items"
  }
]