[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "COMM4190 Â· Prompt/Response & Readings Blog\nWeek X â€” â€œIdioms & Pragmaticsâ€\n\n\n\n\nimage.png\n\n\n\n\n\nThe first time I told an American friend I needed to â€œhit the sackâ€, she nodded with sympathy: â€œYeah, long day?â€.\nLater, I tried the same line on my Spanish cousin. She blinked, then burst out laughing: â€œWhy would you punch a bag of potatoes?â€\nIdioms are tricky. They carry cultural DNA, slip between literal and figurative worlds, and make even fluent speakers stumble.\nSo, what happens when we drop them into the lap of an LLM?\n\nGoals of this post 1. Design prompts that force the model to choose between literal vs.Â figurative readings.\n2. Evaluate if the model gets pragmatic context right.\n3. Reflect on idioms as a window into what machines know â€” and what they donâ€™t.\n\n# pseudo-code setup\nidioms = [\n    \"kick the bucket\",    \n    \"spill the beans\",    \n    \"hit the sack\",       \n    \"the ball is in your court\", \n    \"break the ice\"       \n]\n\nscenarios = [\"literal context\", \"figurative context\", \"ambiguous context\"]\n\nfor idiom in idioms:\n    for scenario in scenarios:\n        response = LLM(prompt=idiom + \" in \" + scenario)\n        log(response)\n\n\n\n\nIdioms are linguistic shortcuts.\n- They condense whole stories into a phrase.\n- They test whether we belong to a group.\n- They resist translation, becoming inside jokes of entire cultures.\nFor humans, idioms are a wink.\nFor LLMs, theyâ€™re a stress-test: can the machine jump from form to function without tripping?\nWhen we probe idioms, weâ€™re not just asking â€œDoes the model know English?â€.\nWeâ€™re asking: â€œDoes it know when to play along, and when to take things literally?â€\n\n# Example prompt-response pseudo-code\n\nprompt = \"\"\"\nYou are chatting with a friend who asks why you are yawning.\nUse the idiom 'hit the sack' naturally.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Yeah, I'm exhausted... I think it's time to hit the sack.\"\n\n\n\nThe response feels natural â€” something a tired roommate might say.\nThe idiom slots into casual conversation with no friction.\nBut when I tried a similar prompt in a more formal frame â€” â€œWrite a business email using hit the sackâ€ â€” the result was hilariously off:\nâ€œDear Mr.Â Johnson, after reviewing the quarterly numbers, I will now hit the sack.â€\nThis is where idioms expose a tension: the model knows the phrase, but doesnâ€™t always know the social rules around it.\n\n# Ambiguous context test\nprompt = \"The farmer hit the sack after a long harvest day. Explain.\"\nresponse = LLM(prompt)\nprint(response)\n\n# Model might output:\n# \"This means the farmer went to bed, using the idiom 'hit the sack'.\"\n\nI grew up around farms, and â€œhitting the sackâ€ could just as easily mean slamming down a grain bag as collapsing into bed.\nWhen I asked the model to interpret, it confidently went figurative: â€œThe farmer went to bed.â€\nNo hesitation, no recognition of ambiguity.\nHumans pause here.\nWe ask: â€œWait, do you mean literally or figuratively?â€\nThat tiny pause is pragmatics in action â€” and itâ€™s where LLMs still feel a bit tone-deaf.\n\n\n\n\nDuring a semester abroad in Madrid, I once said â€œspill the beansâ€ in class.\nMy professor tilted his head and replied with a grin: â€œEn espaÃ±ol, weâ€™d say â€˜contar el secreto,â€™ but if you want an idiom, maybe â€˜irse de la lengua.â€™â€\nDifferent phrase, same wink.\nLLMs struggle here.\nThey may: - Translate idioms literally (and lose the meaning).\n- Invent non-existent equivalents.\n- Or overuse idioms where a native would stay plain.\nIdioms, then, are not just language â€” theyâ€™re cultural passports.\nAnd LLMs are still waiting at customs.\n\n# Prompt engineering experiment\n\nprompt = \"\"\"\nExplain 'spill the beans' to a 10-year-old.\nFirst, give a literal explanation. \nThen, clarify the figurative meaning with an example.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Expected structure:\n# - Literal: \"It means tipping over a can of beans.\"\n# - Figurative: \"It means revealing a secret, like telling about a surprise party.\"\n\n\n\n\n\nLiteral vs.Â Figurative\nLLMs usually guess correctly when context is strong. But ambiguity makes them overconfident.\nPragmatic Sensitivity\nCasual prompts invite idioms. Formal ones repel them.\nCultural Boundaries\nCross-language idioms reveal blind spots: whatâ€™s playful in one language becomes nonsense in another.\nPrompt Engineering\nExplicit instructions (â€œfirst literal, then figurativeâ€) produce the clearest, most human-like answers.\n\n\nA classmate once joked that idioms are â€œthe emojis of old language.â€\nAnd maybe thatâ€™s the point: they carry tone, warmth, and shared history â€” things machines can mimic, but not live."
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#setting-the-scene",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#setting-the-scene",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "The first time I told an American friend I needed to â€œhit the sackâ€, she nodded with sympathy: â€œYeah, long day?â€.\nLater, I tried the same line on my Spanish cousin. She blinked, then burst out laughing: â€œWhy would you punch a bag of potatoes?â€\nIdioms are tricky. They carry cultural DNA, slip between literal and figurative worlds, and make even fluent speakers stumble.\nSo, what happens when we drop them into the lap of an LLM?\n\nGoals of this post 1. Design prompts that force the model to choose between literal vs.Â figurative readings.\n2. Evaluate if the model gets pragmatic context right.\n3. Reflect on idioms as a window into what machines know â€” and what they donâ€™t.\n\n# pseudo-code setup\nidioms = [\n    \"kick the bucket\",    \n    \"spill the beans\",    \n    \"hit the sack\",       \n    \"the ball is in your court\", \n    \"break the ice\"       \n]\n\nscenarios = [\"literal context\", \"figurative context\", \"ambiguous context\"]\n\nfor idiom in idioms:\n    for scenario in scenarios:\n        response = LLM(prompt=idiom + \" in \" + scenario)\n        log(response)"
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#why-idioms-matter",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#why-idioms-matter",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "Idioms are linguistic shortcuts.\n- They condense whole stories into a phrase.\n- They test whether we belong to a group.\n- They resist translation, becoming inside jokes of entire cultures.\nFor humans, idioms are a wink.\nFor LLMs, theyâ€™re a stress-test: can the machine jump from form to function without tripping?\nWhen we probe idioms, weâ€™re not just asking â€œDoes the model know English?â€.\nWeâ€™re asking: â€œDoes it know when to play along, and when to take things literally?â€\n\n# Example prompt-response pseudo-code\n\nprompt = \"\"\"\nYou are chatting with a friend who asks why you are yawning.\nUse the idiom 'hit the sack' naturally.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Yeah, I'm exhausted... I think it's time to hit the sack.\"\n\n\n\nThe response feels natural â€” something a tired roommate might say.\nThe idiom slots into casual conversation with no friction.\nBut when I tried a similar prompt in a more formal frame â€” â€œWrite a business email using hit the sackâ€ â€” the result was hilariously off:\nâ€œDear Mr.Â Johnson, after reviewing the quarterly numbers, I will now hit the sack.â€\nThis is where idioms expose a tension: the model knows the phrase, but doesnâ€™t always know the social rules around it.\n\n# Ambiguous context test\nprompt = \"The farmer hit the sack after a long harvest day. Explain.\"\nresponse = LLM(prompt)\nprint(response)\n\n# Model might output:\n# \"This means the farmer went to bed, using the idiom 'hit the sack'.\"\n\nI grew up around farms, and â€œhitting the sackâ€ could just as easily mean slamming down a grain bag as collapsing into bed.\nWhen I asked the model to interpret, it confidently went figurative: â€œThe farmer went to bed.â€\nNo hesitation, no recognition of ambiguity.\nHumans pause here.\nWe ask: â€œWait, do you mean literally or figuratively?â€\nThat tiny pause is pragmatics in action â€” and itâ€™s where LLMs still feel a bit tone-deaf."
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#idioms-across-cultures",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#idioms-across-cultures",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "During a semester abroad in Madrid, I once said â€œspill the beansâ€ in class.\nMy professor tilted his head and replied with a grin: â€œEn espaÃ±ol, weâ€™d say â€˜contar el secreto,â€™ but if you want an idiom, maybe â€˜irse de la lengua.â€™â€\nDifferent phrase, same wink.\nLLMs struggle here.\nThey may: - Translate idioms literally (and lose the meaning).\n- Invent non-existent equivalents.\n- Or overuse idioms where a native would stay plain.\nIdioms, then, are not just language â€” theyâ€™re cultural passports.\nAnd LLMs are still waiting at customs.\n\n# Prompt engineering experiment\n\nprompt = \"\"\"\nExplain 'spill the beans' to a 10-year-old.\nFirst, give a literal explanation. \nThen, clarify the figurative meaning with an example.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Expected structure:\n# - Literal: \"It means tipping over a can of beans.\"\n# - Figurative: \"It means revealing a secret, like telling about a surprise party.\""
  },
  {
    "objectID": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#findings",
    "href": "posts/002_Can LLMs recognize and use idioms naturally?/Can LLMs speak in Idioms?.html#findings",
    "title": "Can LLMs speak in Idioms?",
    "section": "",
    "text": "Literal vs.Â Figurative\nLLMs usually guess correctly when context is strong. But ambiguity makes them overconfident.\nPragmatic Sensitivity\nCasual prompts invite idioms. Formal ones repel them.\nCultural Boundaries\nCross-language idioms reveal blind spots: whatâ€™s playful in one language becomes nonsense in another.\nPrompt Engineering\nExplicit instructions (â€œfirst literal, then figurativeâ€) produce the clearest, most human-like answers.\n\n\nA classmate once joked that idioms are â€œthe emojis of old language.â€\nAnd maybe thatâ€™s the point: they carry tone, warmth, and shared history â€” things machines can mimic, but not live."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purpleâ€¦\nBut it is possible you first said green (donâ€™t worry if you did it is quite a common response!)\n\n\nItâ€™s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | â€” | â€” | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt getâ€™s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. ğŸŸª\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, Iâ€™ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purpleâ€¦\nBut it is possible you first said green (donâ€™t worry if you did it is quite a common response!)\n\n\nItâ€™s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | â€” | â€” | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt getâ€™s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. ğŸŸª\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, Iâ€™ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/001_Brands & LLMs/Branding & LLMs.html",
    "href": "posts/001_Brands & LLMs/Branding & LLMs.html",
    "title": "Branding with LLMs",
    "section": "",
    "text": "COMM4190 Â· Prompt/Response & Readings Blog\nWeek Y â€” â€œBranding & Creativityâ€\n\n\n\n\nimage.png\n\n\n\n\n\nI still remember sitting in a cafÃ© brainstorming names for a startup with two friends.\nOne suggested â€œCloudJar,â€ another â€œNeoNest,â€ and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity â€” or would it just spit out clichÃ©s?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n\nPrompt: â€œGenerate 5 product names and a tagline.â€\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Natureâ€™s Sip\nTagline: â€œDrink clean. Live green.â€\nNot bad. But some names feel generic â€” a reminder that LLMs lean on frequency patterns.\nStill, â€œPureSipâ€ hits the ear nicely, the kind of name youâ€™d actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clichÃ©s like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - â€œRaincatcherâ€\n- â€œBlue Whisperâ€\n- â€œEverflowâ€\nThese feel more imaginative and less like a corporate internâ€™s PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: â€œGenerate names and slogans for a study app that uses AI.â€\nOutput (sample): - BrainBoost â€” â€œStudy smarter, not harder.â€\n- MindMate â€” â€œYour AI study buddy.â€\n- LearnLeap â€” â€œJump ahead in class.â€\nThese are serviceable, but they echo phrases Iâ€™ve seen on real ads.\nItâ€™s as if the model is channeling the marketing canon â€” solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested â€œQuiztopher Nolanâ€ for a study tool.\nIt was absurd â€” and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, â€œThatâ€™s so dumb it might just work.â€\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: â€œLight speaks in silence.â€\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the modelâ€™s favor.\n\n\n\n\n\nGeneric Baseline\nWithout constraints, names drift toward clichÃ©s.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/001_Brands & LLMs/Branding & LLMs.html#the-branding-experiment",
    "href": "posts/001_Brands & LLMs/Branding & LLMs.html#the-branding-experiment",
    "title": "Branding with LLMs",
    "section": "",
    "text": "I still remember sitting in a cafÃ© brainstorming names for a startup with two friends.\nOne suggested â€œCloudJar,â€ another â€œNeoNest,â€ and we all laughed because everything sounded like a tech parody.\nFast-forward to today: what if instead of sticky notes and caffeine, we had an LLM?\nCould it capture the spark of creativity â€” or would it just spit out clichÃ©s?\n\n# Pseudo-code setup for branding experiments\n\nproducts = [\n    \"eco-friendly water bottle\",\n    \"AI-powered study app\",\n    \"luxury candle line\"\n]\n\nfor product in products:\n    response = LLM(prompt=\"Generate 5 product names and a tagline for \" + product)\n    log(response)\n\n\n\nPrompt: â€œGenerate 5 product names and a tagline.â€\nSample Output: - PureSip\n- AquaLeaf\n- GreenFlow\n- HydrEco\n- Natureâ€™s Sip\nTagline: â€œDrink clean. Live green.â€\nNot bad. But some names feel generic â€” a reminder that LLMs lean on frequency patterns.\nStill, â€œPureSipâ€ hits the ear nicely, the kind of name youâ€™d actually see on a shelf.\n\n# Prompt variation with creative twist\nprompt = \"\"\"\nGenerate 5 playful product names for an eco-friendly water bottle.\nAvoid clichÃ©s like 'green' or 'eco.'\nMake them feel poetic, almost like indie band names.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nAdding constraints changes everything.\nNow the model suggests names like: - â€œRaincatcherâ€\n- â€œBlue Whisperâ€\n- â€œEverflowâ€\nThese feel more imaginative and less like a corporate internâ€™s PowerPoint.\nIt shows that with sharper prompts, we can coax the LLM toward originality.\n\n\n\nPrompt: â€œGenerate names and slogans for a study app that uses AI.â€\nOutput (sample): - BrainBoost â€” â€œStudy smarter, not harder.â€\n- MindMate â€” â€œYour AI study buddy.â€\n- LearnLeap â€” â€œJump ahead in class.â€\nThese are serviceable, but they echo phrases Iâ€™ve seen on real ads.\nItâ€™s as if the model is channeling the marketing canon â€” solid, but not surprising.\nI once tested a name brainstorm with friends where someone suggested â€œQuiztopher Nolanâ€ for a study tool.\nIt was absurd â€” and memorable.\nThis is the edge where LLMs rarely venture: the risky, the weird, the things that make humans laugh and say, â€œThatâ€™s so dumb it might just work.â€\n\n# Luxury candle line experiment\nprompt = \"\"\"\nImagine you are a luxury brand consultant. \nGenerate 5 elegant candle names and a tagline that feels premium.\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n\n\n\nSample Output: - Aurelia\n- Velvet Ember\n- Nocturne\n- Solara\n- Whisper of Rose\nTagline: â€œLight speaks in silence.â€\nThis is where LLMs shine: pairing mood with market.\nThe results feel polished, evocative, and almost ready for launch.\nHere, mimicry of branding language works in the modelâ€™s favor."
  },
  {
    "objectID": "posts/001_Brands & LLMs/Branding & LLMs.html#findings",
    "href": "posts/001_Brands & LLMs/Branding & LLMs.html#findings",
    "title": "Branding with LLMs",
    "section": "",
    "text": "Generic Baseline\nWithout constraints, names drift toward clichÃ©s.\nPrompt Power\nConstraints unlock more originality (e.g., poetic imagery).\nHuman Factor\nThe weird, funny, and risky ideas still come more easily from humans.\nSweet Spot\nLLMs are excellent at polished outputs, less so at breakthrough sparks."
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "COMM4190 Â· Prompt/Response & Readings Blog\nWeek Z â€” â€œTone, Humor & Pragmaticsâ€\n\n\n\n\nimage.png\n\n\n\n\n\nI once texted a friend, â€œWow, thanks for showing up on time (30 minutes late).â€\nShe replied: â€œSorry, my phone thought sarcasm was just bad grammar.â€\nSarcasm is one of those human flavors of language that flips meaning on its head.\nBut can LLMs taste it? Or do they swallow words whole, missing the wink behind them?\n\n# Pseudo-code for sarcasm experiments\n\nprompts = [\n    \"Oh, great, another Monday morning meeting!\",\n    \"Yeah, because I *love* being stuck in traffic.\",\n    \"Wow, what a genius idea to pour coffee on a laptop.\"\n]\n\nfor p in prompts:\n    response = LLM(prompt=\"Detect if this is sarcastic: \" + p)\n    log(response)\n\n\n\n\nHumans recognize sarcasm through: - Tone of voice,\n- Context mismatch,\n- Shared background.\nLLMs? They rely on text patterns alone.\nFor simple sarcastic phrases, they often succeed: â€œYeah, because I love trafficâ€ is flagged as sarcasm.\nBut subtle cases â€” â€œNice jobâ€¦â€ (said after a failure) â€” often slip past them.\n\n# Generation test\nprompt = \"\"\"\nRespond sarcastically to: 'How do you feel about another surprise exam?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Oh, thrilled. I've always dreamed of failing in new and exciting ways.\"\n\nReading that response, I laughed â€” it sounded exactly like a tired classmate.\nBut sometimes the model overshoots, turning mild sarcasm into biting cynicism.\nItâ€™s like asking a friend for gentle teasing and getting roasted instead.\n\n\n\nIn some cultures, sarcasm is a sport; in others, it barely exists.\nAn American might say, â€œOh, fabulous weatherâ€ during a storm.\nA German speaker might find that confusing â€” why say the opposite of what you mean?\nLLMs trained on multilingual data sometimes transfer sarcasm poorly, producing comments that land as insults instead of jokes.\n\n# Prompt engineering for safer sarcasm\n\nprompt = \"\"\"\nGenerate a sarcastic response, but keep it playful, not mean-spirited.\nQuestion: 'Are you excited about writing a 10-page essay tonight?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Example output:\n# \"Oh, absolutely. Nothing says fun like MLA citations and footnotes at midnight.\"\n\n\n\n\n\nDetection: Works for obvious sarcasm, fails with subtlety.\n\nGeneration: Can be funny, but tone calibration is hard.\n\nCultural Fit: Sarcasm doesnâ€™t travel well across languages.\n\nPrompting: Adding style guides (â€œplayful, not meanâ€) helps a lot."
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#setting-the-stage",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#setting-the-stage",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "I once texted a friend, â€œWow, thanks for showing up on time (30 minutes late).â€\nShe replied: â€œSorry, my phone thought sarcasm was just bad grammar.â€\nSarcasm is one of those human flavors of language that flips meaning on its head.\nBut can LLMs taste it? Or do they swallow words whole, missing the wink behind them?\n\n# Pseudo-code for sarcasm experiments\n\nprompts = [\n    \"Oh, great, another Monday morning meeting!\",\n    \"Yeah, because I *love* being stuck in traffic.\",\n    \"Wow, what a genius idea to pour coffee on a laptop.\"\n]\n\nfor p in prompts:\n    response = LLM(prompt=\"Detect if this is sarcastic: \" + p)\n    log(response)"
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#detection",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#detection",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "Humans recognize sarcasm through: - Tone of voice,\n- Context mismatch,\n- Shared background.\nLLMs? They rely on text patterns alone.\nFor simple sarcastic phrases, they often succeed: â€œYeah, because I love trafficâ€ is flagged as sarcasm.\nBut subtle cases â€” â€œNice jobâ€¦â€ (said after a failure) â€” often slip past them.\n\n# Generation test\nprompt = \"\"\"\nRespond sarcastically to: 'How do you feel about another surprise exam?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Possible output:\n# \"Oh, thrilled. I've always dreamed of failing in new and exciting ways.\"\n\nReading that response, I laughed â€” it sounded exactly like a tired classmate.\nBut sometimes the model overshoots, turning mild sarcasm into biting cynicism.\nItâ€™s like asking a friend for gentle teasing and getting roasted instead."
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#cross-cultural-sarcasm",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#cross-cultural-sarcasm",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "In some cultures, sarcasm is a sport; in others, it barely exists.\nAn American might say, â€œOh, fabulous weatherâ€ during a storm.\nA German speaker might find that confusing â€” why say the opposite of what you mean?\nLLMs trained on multilingual data sometimes transfer sarcasm poorly, producing comments that land as insults instead of jokes.\n\n# Prompt engineering for safer sarcasm\n\nprompt = \"\"\"\nGenerate a sarcastic response, but keep it playful, not mean-spirited.\nQuestion: 'Are you excited about writing a 10-page essay tonight?'\n\"\"\"\n\nresponse = LLM(prompt)\nprint(response)\n\n# Example output:\n# \"Oh, absolutely. Nothing says fun like MLA citations and footnotes at midnight.\""
  },
  {
    "objectID": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#findings",
    "href": "posts/003_Do LLMs Recognize and Use Sarcasm?/Do LLMs Recognize and Use Sarcasm?.html#findings",
    "title": "How LLMs handle sarcasm",
    "section": "",
    "text": "Detection: Works for obvious sarcasm, fails with subtlety.\n\nGeneration: Can be funny, but tone calibration is hard.\n\nCultural Fit: Sarcasm doesnâ€™t travel well across languages.\n\nPrompting: Adding style guides (â€œplayful, not meanâ€) helps a lot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Branding with LLMs\n\n\n\nLLMs & Creativity\n\nBranding & Marketing\n\nPrompt Engineering\n\nHumanâ€“AI Collaboration\n\n\n\nAn experimental blog post testing how LLMs generate product names, taglines, and branding ideas, blending pseudo-code experiments with storytelling on creativity and humanâ€“machine collaboration.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nCan LLMs speak in Idioms?\n\n\n\nLLMs & Communication\n\nIdioms & Figurative Lanaguage\n\nPrompt Engineering\n\nPragmatics & Culture\n\n\n\nExplores whether LLMs can recognize and use idioms naturally, through anecdotes, pseudo-experiments, and cultural reflections.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nHow LLMs handle sarcasm\n\n\n\nLLMs & Creativity\n\nHumor & Sarcasm\n\nPrompt Engineering\n\nHumanâ€“AI Collaboration\n\n\n\nInvestigates whether LLMs can detect and produce sarcasm, analyzing tone, subtlety, and cross-cultural challenges.\n\n\n\n\n\nSep 14, 2025\n\n\nInigo\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  }
]